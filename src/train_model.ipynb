{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gdTXDKm04Xrw"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-aVkSSB42NG"},"outputs":[],"source":["from os import path\n","\n","root_path = \"/content/gdrive/Shareddrives/BA/R Python/Đồ án/src/\"\n","dataset_path = path.join(root_path, \"dataset\")\n","annotated_path = path.join(dataset_path, 'annotated')\n","\n","!ls \"$dataset_path\"\n","!ls \"$annotated_path\""]},{"cell_type":"markdown","metadata":{"id":"0Jp02BmqRg3j"},"source":["# Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7837,"status":"ok","timestamp":1714956113448,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"fNEaYLME5Q6n","outputId":"92c70575-dc3d-4653-9bae-462cd08354a5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Sheet1':      rating_star        cmtid       itemid  \\\n"," 0              5  12486152255  23676957049   \n"," 1              5  12999867370  23754591984   \n"," 2              4  12360238836  23676957049   \n"," 3              5  12877630757  23182460672   \n"," 4              1  12154882262  16694691238   \n"," ..           ...          ...          ...   \n"," 995            5  10688383146  20953247691   \n"," 996            5  11843187231  19872577573   \n"," 997            4  11096357262  14395790114   \n"," 998            5  13093513831  20682429404   \n"," 999            5  13341569864  21695044273   \n"," \n","                                                   name  \\\n"," 0    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 1    Quần jean ống suông rách gối , Quần bò dáng su...   \n"," 2    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 3    Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n"," 4    ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," ..                                                 ...   \n"," 995  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n"," 996  Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 997  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 998  Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n"," 999  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," \n","                                                comment  \\\n"," 0    Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n"," 1    Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n"," 2    Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n"," 3    \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n"," 4                                Xấu tốt nhất đừng mua   \n"," ..                                                 ...   \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...   \n"," 996  Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...   \n"," 997  ￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...   \n"," 998  Đẹp tek này k mua thì mua gì nữa hả các bác 10...   \n"," 999                    Đẹp quá trời. Mua lẹ mn ơiiiiii   \n"," \n","                                              clean_cmt  in - out  star  \n"," 0    xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       NaN   NaN  \n"," 1    xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       NaN   NaN  \n"," 2    xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       NaN   NaN  \n"," 3    xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       NaN   NaN  \n"," 4                                xấu tốt nhất đừng mua       NaN   NaN  \n"," ..                                                 ...       ...   ...  \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...       NaN   NaN  \n"," 996  điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...       NaN   NaN  \n"," 997  didika bộ áo thun ngắn tay quần short thể_thao...       NaN   NaN  \n"," 998  đẹp tek này không mua thì mua gì nữa hả các bá...       NaN   NaN  \n"," 999              đẹp quá_trời mua lẹ mọi người ơiiiiii       NaN   NaN  \n"," \n"," [1000 rows x 8 columns],\n"," 'Uyên':      rating_star        cmtid       itemid  \\\n"," 0              5  12486152255  23676957049   \n"," 1              5  12999867370  23754591984   \n"," 2              4  12360238836  23676957049   \n"," 3              5  12877630757  23182460672   \n"," 4              1  12154882262  16694691238   \n"," ..           ...          ...          ...   \n"," 995            5  10688383146  20953247691   \n"," 996            5  11843187231  19872577573   \n"," 997            4  11096357262  14395790114   \n"," 998            5  13093513831  20682429404   \n"," 999            5  13341569864  21695044273   \n"," \n","                                                   name  \\\n"," 0    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 1    Quần jean ống suông rách gối , Quần bò dáng su...   \n"," 2    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 3    Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n"," 4    ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," ..                                                 ...   \n"," 995  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n"," 996  Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 997  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 998  Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n"," 999  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," \n","                                                comment  \\\n"," 0    Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n"," 1    Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n"," 2    Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n"," 3    \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n"," 4                                Xấu tốt nhất đừng mua   \n"," ..                                                 ...   \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...   \n"," 996  Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...   \n"," 997  ￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...   \n"," 998  Đẹp tek này k mua thì mua gì nữa hả các bác 10...   \n"," 999                    Đẹp quá trời. Mua lẹ mn ơiiiiii   \n"," \n","                                              clean_cmt in - out  star  \n"," 0    xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in     5  \n"," 1    xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in     5  \n"," 2    xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in     5  \n"," 3    xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in     5  \n"," 4                                xấu tốt nhất đừng mua       in     1  \n"," ..                                                 ...      ...   ...  \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...       in     5  \n"," 996  điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...       in     5  \n"," 997  didika bộ áo thun ngắn tay quần short thể_thao...      out     0  \n"," 998  đẹp tek này không mua thì mua gì nữa hả các bá...       in     5  \n"," 999              đẹp quá_trời mua lẹ mọi người ơiiiiii       in     5  \n"," \n"," [1000 rows x 8 columns],\n"," 'Tuấn':      rating_star        cmtid       itemid  \\\n"," 0              5  12486152255  23676957049   \n"," 1              5  12999867370  23754591984   \n"," 2              4  12360238836  23676957049   \n"," 3              5  12877630757  23182460672   \n"," 4              1  12154882262  16694691238   \n"," ..           ...          ...          ...   \n"," 995            5  10688383146  20953247691   \n"," 996            5  11843187231  19872577573   \n"," 997            4  11096357262  14395790114   \n"," 998            5  13093513831  20682429404   \n"," 999            5  13341569864  21695044273   \n"," \n","                                                   name  \\\n"," 0    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 1    Quần jean ống suông rách gối , Quần bò dáng su...   \n"," 2    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 3    Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n"," 4    ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," ..                                                 ...   \n"," 995  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n"," 996  Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 997  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 998  Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n"," 999  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," \n","                                                comment  \\\n"," 0    Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n"," 1    Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n"," 2    Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n"," 3    \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n"," 4                                Xấu tốt nhất đừng mua   \n"," ..                                                 ...   \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...   \n"," 996  Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...   \n"," 997  ￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...   \n"," 998  Đẹp tek này k mua thì mua gì nữa hả các bác 10...   \n"," 999                    Đẹp quá trời. Mua lẹ mn ơiiiiii   \n"," \n","                                              clean_cmt in - out  star  \n"," 0    xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in   5.0  \n"," 1    xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in   5.0  \n"," 2    xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in   5.0  \n"," 3    xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in   5.0  \n"," 4                                xấu tốt nhất đừng mua       in   1.0  \n"," ..                                                 ...      ...   ...  \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...       in   5.0  \n"," 996  điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...       in   5.0  \n"," 997  didika bộ áo thun ngắn tay quần short thể_thao...      out   0.0  \n"," 998  đẹp tek này không mua thì mua gì nữa hả các bá...       in   5.0  \n"," 999              đẹp quá_trời mua lẹ mọi người ơiiiiii       in   5.0  \n"," \n"," [1000 rows x 8 columns],\n"," 'Tâm':      rating_star        cmtid       itemid  \\\n"," 0              5  12486152255  23676957049   \n"," 1              5  12999867370  23754591984   \n"," 2              4  12360238836  23676957049   \n"," 3              5  12877630757  23182460672   \n"," 4              1  12154882262  16694691238   \n"," ..           ...          ...          ...   \n"," 995            5  10688383146  20953247691   \n"," 996            5  11843187231  19872577573   \n"," 997            4  11096357262  14395790114   \n"," 998            5  13093513831  20682429404   \n"," 999            5  13341569864  21695044273   \n"," \n","                                                   name  \\\n"," 0    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 1    Quần jean ống suông rách gối , Quần bò dáng su...   \n"," 2    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 3    Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n"," 4    ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," ..                                                 ...   \n"," 995  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n"," 996  Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 997  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 998  Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n"," 999  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," \n","                                                comment  \\\n"," 0    Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n"," 1    Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n"," 2    Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n"," 3    \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n"," 4                                Xấu tốt nhất đừng mua   \n"," ..                                                 ...   \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...   \n"," 996  Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...   \n"," 997  ￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...   \n"," 998  Đẹp tek này k mua thì mua gì nữa hả các bác 10...   \n"," 999                    Đẹp quá trời. Mua lẹ mn ơiiiiii   \n"," \n","                                              clean_cmt in - out  star  \n"," 0    xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in     5  \n"," 1    xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in     5  \n"," 2    xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in     5  \n"," 3    xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in     5  \n"," 4                                xấu tốt nhất đừng mua       in     1  \n"," ..                                                 ...      ...   ...  \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...       in     5  \n"," 996  điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...       in     5  \n"," 997  didika bộ áo thun ngắn tay quần short thể_thao...      out     0  \n"," 998  đẹp tek này không mua thì mua gì nữa hả các bá...       in     5  \n"," 999              đẹp quá_trời mua lẹ mọi người ơiiiiii       in     5  \n"," \n"," [1000 rows x 8 columns],\n"," 'Thư':      rating_star        cmtid       itemid  \\\n"," 0              5  12486152255  23676957049   \n"," 1              5  12999867370  23754591984   \n"," 2              4  12360238836  23676957049   \n"," 3              5  12877630757  23182460672   \n"," 4              1  12154882262  16694691238   \n"," ..           ...          ...          ...   \n"," 995            5  10688383146  20953247691   \n"," 996            5  11843187231  19872577573   \n"," 997            4  11096357262  14395790114   \n"," 998            5  13093513831  20682429404   \n"," 999            5  13341569864  21695044273   \n"," \n","                                                   name  \\\n"," 0    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 1    Quần jean ống suông rách gối , Quần bò dáng su...   \n"," 2    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 3    Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n"," 4    ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," ..                                                 ...   \n"," 995  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n"," 996  Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 997  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 998  Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n"," 999  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," \n","                                                comment  \\\n"," 0    Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n"," 1    Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n"," 2    Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n"," 3    \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n"," 4                                Xấu tốt nhất đừng mua   \n"," ..                                                 ...   \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...   \n"," 996  Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...   \n"," 997  ￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...   \n"," 998  Đẹp tek này k mua thì mua gì nữa hả các bác 10...   \n"," 999                    Đẹp quá trời. Mua lẹ mn ơiiiiii   \n"," \n","                                              clean_cmt in - out  star  \n"," 0    xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in     5  \n"," 1    xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in     5  \n"," 2    xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in     5  \n"," 3    xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in     5  \n"," 4                                xấu tốt nhất đừng mua       in     1  \n"," ..                                                 ...      ...   ...  \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...       in     4  \n"," 996  điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...       in     5  \n"," 997  didika bộ áo thun ngắn tay quần short thể_thao...       in     5  \n"," 998  đẹp tek này không mua thì mua gì nữa hả các bá...       in     5  \n"," 999              đẹp quá_trời mua lẹ mọi người ơiiiiii       in     5  \n"," \n"," [1000 rows x 8 columns],\n"," 'Thuy':      rating_star        cmtid       itemid  \\\n"," 0              5  12486152255  23676957049   \n"," 1              5  12999867370  23754591984   \n"," 2              4  12360238836  23676957049   \n"," 3              5  12877630757  23182460672   \n"," 4              1  12154882262  16694691238   \n"," ..           ...          ...          ...   \n"," 995            5  10688383146  20953247691   \n"," 996            5  11843187231  19872577573   \n"," 997            4  11096357262  14395790114   \n"," 998            5  13093513831  20682429404   \n"," 999            5  13341569864  21695044273   \n"," \n","                                                   name  \\\n"," 0    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 1    Quần jean ống suông rách gối , Quần bò dáng su...   \n"," 2    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 3    Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n"," 4    ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," ..                                                 ...   \n"," 995  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n"," 996  Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 997  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 998  Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n"," 999  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," \n","                                                comment  \\\n"," 0    Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n"," 1    Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n"," 2    Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n"," 3    \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n"," 4                                Xấu tốt nhất đừng mua   \n"," ..                                                 ...   \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...   \n"," 996  Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...   \n"," 997  ￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...   \n"," 998  Đẹp tek này k mua thì mua gì nữa hả các bác 10...   \n"," 999                    Đẹp quá trời. Mua lẹ mn ơiiiiii   \n"," \n","                                              clean_cmt in - out  star  \n"," 0    xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in   5.0  \n"," 1    xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in   5.0  \n"," 2    xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in   5.0  \n"," 3    xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in   5.0  \n"," 4                                xấu tốt nhất đừng mua       in   1.0  \n"," ..                                                 ...      ...   ...  \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...       in   5.0  \n"," 996  điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...       in   5.0  \n"," 997  didika bộ áo thun ngắn tay quần short thể_thao...      out   0.0  \n"," 998  đẹp tek này không mua thì mua gì nữa hả các bá...       in   5.0  \n"," 999              đẹp quá_trời mua lẹ mọi người ơiiiiii       in   5.0  \n"," \n"," [1000 rows x 8 columns],\n"," 'Ánh':      rating_star        cmtid       itemid  \\\n"," 0              5  12486152255  23676957049   \n"," 1              5  12999867370  23754591984   \n"," 2              4  12360238836  23676957049   \n"," 3              5  12877630757  23182460672   \n"," 4              1  12154882262  16694691238   \n"," ..           ...          ...          ...   \n"," 995            5  10688383146  20953247691   \n"," 996            5  11843187231  19872577573   \n"," 997            4  11096357262  14395790114   \n"," 998            5  13093513831  20682429404   \n"," 999            5  13341569864  21695044273   \n"," \n","                                                      2  \\\n"," 0    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 1    Quần jean ống suông rách gối , Quần bò dáng su...   \n"," 2    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n"," 3    Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n"," 4    ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," ..                                                 ...   \n"," 995  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n"," 996  Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 997  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 998  Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n"," 999  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," \n","                                                comment  \\\n"," 0    Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n"," 1    Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n"," 2    Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n"," 3    \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n"," 4                                Xấu tốt nhất đừng mua   \n"," ..                                                 ...   \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...   \n"," 996  Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...   \n"," 997  ￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...   \n"," 998  Đẹp tek này k mua thì mua gì nữa hả các bác 10...   \n"," 999                    Đẹp quá trời. Mua lẹ mn ơiiiiii   \n"," \n","                                              clean_cmt in - out  star  \n"," 0    xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in   5.0  \n"," 1    xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in   5.0  \n"," 2    xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in   5.0  \n"," 3    xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in   5.0  \n"," 4                                xấu tốt nhất đừng mua       in   1.0  \n"," ..                                                 ...      ...   ...  \n"," 995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...       in   5.0  \n"," 996  điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...       in   5.0  \n"," 997  didika bộ áo thun ngắn tay quần short thể_thao...      out   0.0  \n"," 998  đẹp tek này không mua thì mua gì nữa hả các bá...       in   5.0  \n"," 999              đẹp quá_trời mua lẹ mọi người ơiiiiii       in   5.0  \n"," \n"," [1000 rows x 8 columns]}"]},"metadata":{},"execution_count":3}],"source":["import pandas as pd\n","\n","# Reading multiple sheets from an Excel file\n","df_1000 = pd.read_excel(path.join(annotated_path, 'val_annotated_data_1500.xlsx'), engine=\"openpyxl\", sheet_name=None)\n","df_1000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TKOCH-GGRg4"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUALCUnmQR6y"},"outputs":[],"source":["data = {\n","    'in-out': {},\n","    'star': {}\n","}\n","for col in df_1000.keys():\n","  if 'sheet' in col.lower():\n","    continue\n","\n","  for idx, i in enumerate(df_1000[col]['in - out'].tolist()):\n","    if idx not in data['in-out']:\n","      data['in-out'][idx] = []\n","    data['in-out'][idx].append(i)\n","\n","  for idx, i in enumerate(df_1000[col]['star'].tolist()):\n","    if idx not in data['star']:\n","      data['star'][idx] = []\n","    data['star'][idx].append(i)\n","\n","\n","for k, v in data.items():\n","  for i, j in v.items():\n","    data[k][i] = Counter(j).most_common(1)[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1714956113448,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"jYciCT2bTSIf","outputId":"deb6814b-9284-4001-b8a8-1a6fa403a5f6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     rating_star        cmtid       itemid  \\\n","0              5  12486152255  23676957049   \n","1              5  12999867370  23754591984   \n","2              4  12360238836  23676957049   \n","3              5  12877630757  23182460672   \n","4              1  12154882262  16694691238   \n","..           ...          ...          ...   \n","995            5  10688383146  20953247691   \n","996            5  11843187231  19872577573   \n","997            4  11096357262  14395790114   \n","998            5  13093513831  20682429404   \n","999            5  13341569864  21695044273   \n","\n","                                                  name  \\\n","0    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n","1    Quần jean ống suông rách gối , Quần bò dáng su...   \n","2    áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n","3    Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n","4    ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n","..                                                 ...   \n","995  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n","996  Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n","997  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n","998  Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n","999  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n","\n","                                               comment  \\\n","0    Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n","1    Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n","2    Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n","3    \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n","4                                Xấu tốt nhất đừng mua   \n","..                                                 ...   \n","995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...   \n","996  Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...   \n","997  ￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...   \n","998  Đẹp tek này k mua thì mua gì nữa hả các bác 10...   \n","999                    Đẹp quá trời. Mua lẹ mn ơiiiiii   \n","\n","                                             clean_cmt in - out  star  \n","0    xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in   5.0  \n","1    xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in   5.0  \n","2    xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in   5.0  \n","3    xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in   5.0  \n","4                                xấu tốt nhất đừng mua       in   1.0  \n","..                                                 ...      ...   ...  \n","995  do cam trước bị mờ thui chứ bên ngoài áo rất đ...       in   5.0  \n","996  điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...       in   5.0  \n","997  didika bộ áo thun ngắn tay quần short thể_thao...      out   0.0  \n","998  đẹp tek này không mua thì mua gì nữa hả các bá...       in   5.0  \n","999              đẹp quá_trời mua lẹ mọi người ơiiiiii       in   5.0  \n","\n","[1000 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-73e9a902-41ee-47a9-8a45-e678692d0c38\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rating_star</th>\n","      <th>cmtid</th>\n","      <th>itemid</th>\n","      <th>name</th>\n","      <th>comment</th>\n","      <th>clean_cmt</th>\n","      <th>in - out</th>\n","      <th>star</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>12486152255</td>\n","      <td>23676957049</td>\n","      <td>áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...</td>\n","      <td>Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...</td>\n","      <td>xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5</td>\n","      <td>12999867370</td>\n","      <td>23754591984</td>\n","      <td>Quần jean ống suông rách gối , Quần bò dáng su...</td>\n","      <td>Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...</td>\n","      <td>xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>12360238836</td>\n","      <td>23676957049</td>\n","      <td>áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...</td>\n","      <td>Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...</td>\n","      <td>xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>12877630757</td>\n","      <td>23182460672</td>\n","      <td>Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...</td>\n","      <td>\"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....</td>\n","      <td>xinh lắm luôn á mọi người ơi chất vải mềm lêm ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>12154882262</td>\n","      <td>16694691238</td>\n","      <td>ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...</td>\n","      <td>Xấu tốt nhất đừng mua</td>\n","      <td>xấu tốt nhất đừng mua</td>\n","      <td>in</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>5</td>\n","      <td>10688383146</td>\n","      <td>20953247691</td>\n","      <td>Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...</td>\n","      <td>do cam trước bị mờ thui chứ bên ngoài áo rất đ...</td>\n","      <td>do cam trước bị mờ thui chứ bên ngoài áo rất đ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>5</td>\n","      <td>11843187231</td>\n","      <td>19872577573</td>\n","      <td>Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...</td>\n","      <td>Điện thoại cùi bắp nên chụp hơi mờ nhưng mà qu...</td>\n","      <td>điện_thoại cùi bắp nên chụp hơi mờ nhưng_mà qu...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>4</td>\n","      <td>11096357262</td>\n","      <td>14395790114</td>\n","      <td>DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...</td>\n","      <td>￼DIDIKA Bộ Áo Thun Ngắn Tay + Quần Short Thể T...</td>\n","      <td>didika bộ áo thun ngắn tay quần short thể_thao...</td>\n","      <td>out</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>5</td>\n","      <td>13093513831</td>\n","      <td>20682429404</td>\n","      <td>Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...</td>\n","      <td>Đẹp tek này k mua thì mua gì nữa hả các bác 10...</td>\n","      <td>đẹp tek này không mua thì mua gì nữa hả các bá...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>5</td>\n","      <td>13341569864</td>\n","      <td>21695044273</td>\n","      <td>Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...</td>\n","      <td>Đẹp quá trời. Mua lẹ mn ơiiiiii</td>\n","      <td>đẹp quá_trời mua lẹ mọi người ơiiiiii</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73e9a902-41ee-47a9-8a45-e678692d0c38')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-73e9a902-41ee-47a9-8a45-e678692d0c38 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-73e9a902-41ee-47a9-8a45-e678692d0c38');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-842b9e7c-6601-460c-b664-d868abbec42e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-842b9e7c-6601-460c-b664-d868abbec42e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-842b9e7c-6601-460c-b664-d868abbec42e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"clean_df","summary":"{\n  \"name\": \"clean_df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"rating_star\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cmtid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1228420005,\n        \"min\": 2980655249,\n        \"max\": 13346598120,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          11950834618,\n          12727916674,\n          13094399482\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4970703909,\n        \"min\": 1592182206,\n        \"max\": 25450823380,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          13908068019,\n          23179163007,\n          17599311193\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"\\u00c1o Sweater N\\u1ec9 Nam N\\u1eef Raplang 3 S\\u1ecdc Tay Form R\\u1ed9ng Ulzzang\",\n          \"A\\u0301o N\\u01b0\\u0303 Sweater Sticker, A\\u0301o N\\u1ec9 N\\u01b0\\u0303 Ch\\u00e2\\u0301t Ni\\u0309 Ta\\u0300u In Hi\\u0300nh 5D Si\\u00eau Cute Hottrend 2023\",\n          \"Ch\\u00e2n v\\u00e1y d\\u00e1ng ng\\u1eafn ly to hottrend n\\u1eef t\\u00ednh, Ch\\u00e2n v\\u00e1y n\\u1eef tennis b\\u1ea3n ly to\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"M\\u00e0u s\\u1eafc:\\u0111en\\nCh\\u1ea5t li\\u1ec7u:v\\u1ea3i\\n\\u0110\\u00fang v\\u1edbi m\\u00f4 t\\u1ea3:\\u0111\\u00fang v\\u1edbi m\\u00f4 t\\u1ea3 nh\\u01b0ng m\\u00e0 v\\u1ea3i kh\\u00f4ng \\u0111\\u01b0\\u1ee3c m\\u1ec1m m\\u1ea1i cho l\\u1eafm\",\n          \"\\u0110\\u00fang v\\u1edbi m\\u00f4 t\\u1ea3:ok\\nCh\\u1ea5t li\\u1ec7u:ok\\nM\\u00e0u s\\u1eafc:\\u0111wn\\n\\nH\\u01a1i ng\\u1eafn\",\n          \"\\u0110\\u00fang v\\u1edbi m\\u00f4 t\\u1ea3:Nhu tr\\u00ean h\\u00ecnh r\\u1ed3i\\nCh\\u1ea5t li\\u1ec7u:Len t\\u0103m\\nM\\u00e0u s\\u1eafc:m\\u00e0u tr\\u1eafng\\n\\n\\u1ea2nh th\\u00ec \\u0111\\u00fang nh\\u01b0 tr\\u00ean h\\u00ecnh, giao h\\u00e0ng nhanh, \\u0111\\u00f3ng g\\u00f3i c\\u1ea9n th\\u1eadn. \\u0110o\\u1ea1n n\\u00e0y m\\u00ecnh cho 10 \\u0111i\\u1ec3m\\nNh\\u01b0ng m\\u00e0 m\\u00ecnh m\\u1eb7c th\\u00ec n\\u00f3 h\\u01a1i ng\\u1eafn, kh\\u00f4ng \\u0111\\u01b0\\u1ee3c d\\u00e0i gi\\u1ed1ng nh\\u01b0 m\\u1ea5y t\\u1ec9 t\\u1ec9 b\\u00ean trung, h\\u01a1n n\\u1eefa c\\u00e1i ph\\u1ea7n b\\u00ean tr\\u00ean vai \\u00e1o, n\\u00f3 qu\\u00e1 ng\\u1eafn \\u0111\\u1ec3 \\u0111i\\u1ec1u ch\\u1ec9nh \\u0111\\u1ed9 h\\u1edf, m\\u1eb7c c\\u00f3 ch\\u00fat h\\u01a1i ch\\u1eadt v\\u00e0 kh\\u00f4ng tho\\u1ea3i m\\u00e1i n\\u1eefa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_cmt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"m\\u00e0u s\\u1eafc \\u0111en ch\\u1ea5t_li\\u1ec7u v\\u1ea3i \\u0111\\u00fang v\\u1edbi m\\u00f4_t\\u1ea3 \\u0111\\u00fang v\\u1edbi m\\u00f4_t\\u1ea3 nh\\u01b0ng_m\\u00e0 v\\u1ea3i kh\\u00f4ng \\u0111\\u01b0\\u1ee3c m\\u1ec1m_m\\u1ea1i cho l\\u1eafm\",\n          \"\\u0111\\u00fang v\\u1edbi m\\u00f4_t\\u1ea3 ok ch\\u1ea5t_li\\u1ec7u ok m\\u00e0u_s\\u1eafc \\u0111wn h\\u01a1i ng\\u1eafn\",\n          \"\\u0111\\u00fang v\\u1edbi m\\u00f4_t\\u1ea3 nhu tr\\u00ean h\\u00ecnh r\\u1ed3i ch\\u1ea5t_li\\u1ec7u len t\\u0103m m\\u00e0u_s\\u1eafc m\\u00e0u tr\\u1eafng \\u1ea3nh th\\u00ec \\u0111\\u00fang nh\\u01b0 tr\\u00ean h\\u00ecnh giao h\\u00e0ng nhanh \\u0111\\u00f3ng_g\\u00f3i c\\u1ea9n_th\\u1eadn \\u0111o\\u1ea1n n\\u00e0y m\\u00ecnh cho 10 \\u0111i\\u1ec3m nh\\u01b0ng_m\\u00e0 m\\u00ecnh m\\u1eb7c th\\u00ec n\\u00f3 h\\u01a1i ng\\u1eafn kh\\u00f4ng \\u0111\\u01b0\\u1ee3c d\\u00e0i gi\\u1ed1ng nh\\u01b0 m\\u1ea5y t\\u1ec9 t\\u1ec9 b\\u00ean trung h\\u01a1n_n\\u1eefa c\\u00e1i ph\\u1ea7n b\\u00ean tr\\u00ean vai \\u00e1o n\\u00f3 qu\\u00e1 ng\\u1eafn \\u0111\\u1ec3 \\u0111i\\u1ec1u_ch\\u1ec9nh \\u0111\\u1ed9 h\\u1edf m\\u1eb7c c\\u00f3 ch\\u00fat h\\u01a1i ch\\u1eadt v\\u00e0 kh\\u00f4ng tho\\u1ea3i_m\\u00e1i n\\u1eefa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in - out\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"out\",\n          \"in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"star\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.663132589425753,\n        \"min\": 0.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          5.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}],"source":["clean_df = df_1000['Uyên'].copy(deep = True)\n","clean_df['in - out'] = list(data['in-out'].values())\n","clean_df['star'] = list(data['star'].values())\n","clean_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25478,"status":"ok","timestamp":1714956138912,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"mO-XApX-Pewk","outputId":"b78ac9ea-ad79-4eda-9434-a1add7fa8426"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'src':        rating_star        cmtid       itemid  \\\n"," 0                1  13287131014  22151497601   \n"," 1                1  12316483825  23970387156   \n"," 2                1  12583905239  17695861884   \n"," 3                1  12467199872  19872577573   \n"," 4                1  13290030680  23200225983   \n"," ...            ...          ...          ...   \n"," 13298            1  12198892833  22313943551   \n"," 13299            2  13257460773  19093790478   \n"," 13300            1  13061370615   7141012796   \n"," 13301            5  13194167688  21695044273   \n"," 13302            4  10421157301   6888422353   \n"," \n","                                                     name  \\\n"," 0      Quần Jeans Nữ Ống Loe , Quần Bò Nữ Ống Loe Tua...   \n"," 1      Quần jeans ống loe màu bạc lưng cao co giãn ha...   \n"," 2      Áo khoác Nỉ Crotop Hàn dáng rộng siêu xinh , Á...   \n"," 3      Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 4      áo khoác nỉ hoodie form rộng mũ 2 màu in logo ...   \n"," ...                                                  ...   \n"," 13298  💛 Loại Đẹp 💛 Tất Nam Nữ Cổ Cao Cổ Ngắn Thể Tha...   \n"," 13299  Áo khoác nỉ croptop Keith House dáng rộng siêu...   \n"," 13300  Áo Thun Cổ Tròn Lệch Vai Xoắn Eo Tôn Dáng Nhiề...   \n"," 13301  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," 13302  Quần Ống Rộng Cullotes Vải Nữ Cạp Cao Dài Suôn...   \n"," \n","                                                  comment  \\\n"," 0      Đúng với mô tả:khong dung\\nChất liệu:vải\\nMàu ...   \n"," 1                  May bị lỗi đước quấn lệch qua một bên   \n"," 2      K biết luôn, k có giao cái này. Nên ko nhận xé...   \n"," 3      Màu sắc:trắng\\nChất liệu:vải mềm\\nĐúng với mô ...   \n"," 4      Đúng với mô tả:sai hoàn toàn\\n\\nmk đặt áo này ...   \n"," ...                                                  ...   \n"," 13298                                 24k 1 đôi mẹ ơi=))   \n"," 13299                           150k cái áo chất quá xấu   \n"," 13300  🥲 ảnh màu đen mà nhận màu xanh than,fom áo đẹp...   \n"," 13301  😎Hàng đóng gói rất đẹp. Quả nhiên người bán đã...   \n"," 13302  🔥🔥🔥Tặng 100 Khách Giá Gốc Hôm Nay!!!\\n.ad báo ...   \n"," \n","                                                clean_cmt  \n"," 0      đúng với mô_tả không dung chất_liệu vải màu_sắ...  \n"," 1                  may bị lỗi đước quấn lệch qua một bên  \n"," 2      không biết luôn không có giao cái này nên khôn...  \n"," 3      màu_sắc trắng chất_liệu vải mềm đúng với mô_tả...  \n"," 4      đúng với mô_tả sai hoàn_toàn mình đặt áo này s...  \n"," ...                                                  ...  \n"," 13298                             24 nghìn một đôi mẹ ơi  \n"," 13299                      150 nghìn cái áo chất quá xấu  \n"," 13300  <emoji> smiling face with tear </emoji> ảnh mà...  \n"," 13301  <emoji> smiling face with sunglasses </emoji> ...  \n"," 13302  <emoji> fire </emoji> <emoji> fire </emoji> <e...  \n"," \n"," [13303 rows x 6 columns],\n"," 'Uyên':       rating_star        cmtid       itemid  \\\n"," 0               1  13287131014  22151497601   \n"," 1               1  12316483825  23970387156   \n"," 2               1  12583905239  17695861884   \n"," 3               1  12467199872  19872577573   \n"," 4               1  13290030680  23200225983   \n"," ...           ...          ...          ...   \n"," 2212            2  13085772342  21135778838   \n"," 2213            2  12282464808  18281471951   \n"," 2214            2  10398914825  19061220384   \n"," 2215            2  13313601444  21135778838   \n"," 2216            2  13312346294  19061220384   \n"," \n","                                                    name  \\\n"," 0     Quần Jeans Nữ Ống Loe , Quần Bò Nữ Ống Loe Tua...   \n"," 1     Quần jeans ống loe màu bạc lưng cao co giãn ha...   \n"," 2     Áo khoác Nỉ Crotop Hàn dáng rộng siêu xinh , Á...   \n"," 3     Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," 4     áo khoác nỉ hoodie form rộng mũ 2 màu in logo ...   \n"," ...                                                 ...   \n"," 2212  Áo nỉ hoodie nữ form rộng cute hàn quốc CaMa S...   \n"," 2213  Tất Nam Châm Hình Đôi Mắt Hoạt Hình Xinh Xắn C...   \n"," 2214                                Áo khoá nỉ 336 PM11   \n"," 2215  Áo nỉ hoodie nữ form rộng cute hàn quốc CaMa S...   \n"," 2216                                Áo khoá nỉ 336 PM11   \n"," \n","                                                 comment  \\\n"," 0     Đúng với mô tả:khong dung\\nChất liệu:vải\\nMàu ...   \n"," 1                 May bị lỗi đước quấn lệch qua một bên   \n"," 2     K biết luôn, k có giao cái này. Nên ko nhận xé...   \n"," 3     Màu sắc:trắng\\nChất liệu:vải mềm\\nĐúng với mô ...   \n"," 4     Đúng với mô tả:sai hoàn toàn\\n\\nmk đặt áo này ...   \n"," ...                                                 ...   \n"," 2212  Đúng với mô tả:Nó nhỏ hơn tôi nghĩ so với toii...   \n"," 2213  Đúng với mô tả:Shop giao thiếu một đôi và mình...   \n"," 2214          Chất liệu:mỏng quá\\nMàu sắc:ok\\n\\nNhỏ quá   \n"," 2215  Vải mỏng quá, không lên ham rẻ , chất xấu xổm ...   \n"," 2216  Chất liệu:vải mỏng\\nMàu sắc:be\\nĐúng với mô tả...   \n"," \n","                                               clean_cmt in - out  star  \n"," 0     đúng với mô_tả không dung chất_liệu vải màu_sắ...       in     2  \n"," 1                 may bị lỗi đước quấn lệch qua một bên       in     2  \n"," 2     không biết luôn không có giao cái này nên khôn...      out     0  \n"," 3     màu_sắc trắng chất_liệu vải mềm đúng với mô_tả...       in     4  \n"," 4     đúng với mô_tả sai hoàn_toàn mình đặt áo này s...      out     0  \n"," ...                                                 ...      ...   ...  \n"," 2212  đúng với mô_tả nó nhỏ hơn tôi nghĩ so với toii...       in     3  \n"," 2213  đúng với mô_tả shop giao thiếu một đôi và mình...      out     0  \n"," 2214              chất_liệu mỏng quá màu_sắc ok nhỏ quá       in     2  \n"," 2215  vải mỏng quá không lên ham rẻ chất xấu xổm lôn...       in     1  \n"," 2216  chất_liệu vải mỏng màu_sắc be đúng với mô_tả k...       in     2  \n"," \n"," [2217 rows x 8 columns],\n"," 'Tuấn':       rating_star        cmtid       itemid  \\\n"," 0               2  13178662228   7141012796   \n"," 1               2  13207852040  17398550424   \n"," 2               2  13126172440   7141012796   \n"," 3               2  10479036096  18267407427   \n"," 4               2  13080286883  19872577573   \n"," ...           ...          ...          ...   \n"," 2212            3  12920411037  23849591308   \n"," 2213            3  12224320974  19776807429   \n"," 2214            3  12899026384  17695861884   \n"," 2215            3  11955078134  23970387156   \n"," 2216            3  10566072058  20953247691   \n"," \n","                                                    name  \\\n"," 0     Áo Thun Cổ Tròn Lệch Vai Xoắn Eo Tôn Dáng Nhiề...   \n"," 1     Áo khoác thể thao 3 sọc logo Das chất nỉ bông ...   \n"," 2     Áo Thun Cổ Tròn Lệch Vai Xoắn Eo Tôn Dáng Nhiề...   \n"," 3     ÁO TRỄ VAI NGANG GÂN XẺ TRƯỚC ĐỎ TẾT TƯƠI MỚI ...   \n"," 4     Quần Suông Nữ Dáng Dài Ống Rộng Lưng Cao Thun ...   \n"," ...                                                 ...   \n"," 2212  Quần bò jean nữ ống loe co giãn jeans cạp cao ...   \n"," 2213  Quần Ống Rộng Túi Vuông Cơi Trước Dây Nơ Cách ...   \n"," 2214  Áo khoác Nỉ Crotop Hàn dáng rộng siêu xinh , Á...   \n"," 2215  Quần jeans ống loe màu bạc lưng cao co giãn ha...   \n"," 2216  Áo Len Tăm Nữ Ôm Body Cổ 5 Phân Mềm Mịn Hàng Q...   \n"," \n","                                                 comment  \\\n"," 0     Màu sắc:trang\\nĐúng với mô tả:tạm\\n\\nmình mặc ...   \n"," 1     Xấu quắc… vải xấu…. Po k dãn j het trơn …….đún...   \n"," 2     Chất liệu:vải mỏng\\nMàu sắc:như hình\\n\\nMình m...   \n"," 3     Áo rách dính bẩn không biết giặt ra hông nữa. ...   \n"," 4     Màu hồng kem mặc rất lộ mng cân nhắc, mình nhắ...   \n"," ...                                                 ...   \n"," 2212  88k k đòi hỏi nhiều nhưng mà bạn nào cao với c...   \n"," 2213  Chất liệu:vải\\nĐúng với mô tả:ko đúng lắm\\nMàu...   \n"," 2214  Màu sắc:kem\\nĐúng với mô tả:đúng như mô tả\\n\\n...   \n"," 2215  Chất liệu:bò\\nMàu sắc:kb\\nĐúng với mô tả:kh gi...   \n"," 2216  áo ráchhhh, =))) quá là buồn, cho 3 sao thôi, ...   \n"," \n","                                               clean_cmt in - out star  \n"," 0     màu_sắc trang đúng với mô_tả tạm mình mặc rộng...       in    3  \n"," 1       xấu quắc … vải xấu … po không dãn gì het trơn …       in    1  \n"," 2     chất_liệu vải mỏng màu_sắc như hình mình mua v...       in    2  \n"," 3     áo rách dính bẩn không biết giặt ra hông nữa t...       in    1  \n"," 4     màu hồng kem mặc rất lộ mọi người cân_nhắc mìn...       in    3  \n"," ...                                                 ...      ...  ...  \n"," 2212  88 nghìn không đòi_hỏi nhiều nhưng_mà bạn nào ...       in    1  \n"," 2213  chất_liệu vải đúng với mô_tả không đúng lắm mà...       in    3  \n"," 2214  màu_sắc kem đúng với mô_tả đúng như mô_tả set ...       in    2  \n"," 2215  chất_liệu bò màu_sắc kb đúng với mô_tả không g...       in    2  \n"," 2216  áo ráchhhh quá là buồn cho ba sao thôi giao cũ...       in    1  \n"," \n"," [2217 rows x 8 columns],\n"," 'Ánh':       rating_star        cmtid       itemid  \\\n"," 0               3  10691309761  14395790114   \n"," 1               3  12977780186  19082573960   \n"," 2               3  12119007334  16694691238   \n"," 3               3  12536069861  17695861884   \n"," 4               3  12775863132  22148769772   \n"," ...           ...          ...          ...   \n"," 2212            4  12618774634  10656336686   \n"," 2213            4  13307229809  22582269266   \n"," 2214            4  13229618830  13908068019   \n"," 2215            4  13323872425  21693577588   \n"," 2216            4  13246267673  21492358483   \n"," \n","                                                    name  \\\n"," 0     DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 1     Áo Khoác Hoodie Zip Chất Nỉ Bông Mũ 2 Lớp In C...   \n"," 2     ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," 3     Áo khoác Nỉ Crotop Hàn dáng rộng siêu xinh , Á...   \n"," 4     Áo thun local brand ONTOP Ripple, Áo thun Unis...   \n"," ...                                                 ...   \n"," 2212  Pijama tay dài cổ vuông bộ đồ ngủ nữ pijama ti...   \n"," 2213  Áo Nỉ Sweater Nữ Form Rộng Vải Nỉ Bông Dày Dặn...   \n"," 2214  Áo Sweater Nỉ Nam Nữ Raplang 3 Sọc Tay Form Rộ...   \n"," 2215  [RẺ VÔ ĐỊCH] Áo Nỉ Sweater Chất Nỉ Cotton In N...   \n"," 2216  Áo khoác gió nữ lót lông cừu, vải ngoài tráng ...   \n"," \n","                                                 comment  \\\n"," 0                                           Bình thường   \n"," 1     Mỏng, phù hợp giá khi đã được sale còn 55k. Mì...   \n"," 2     Chất liệu:vải k đẹp bóng bóng\\nĐúng với mô tả:...   \n"," 3                       Áo bẩn quá, coi như hỏng cái áo   \n"," 4     Mua lúc đang sale 29k thì thất vọng quá vải mỏ...   \n"," ...                                                 ...   \n"," 2212  Màu sắc:đen\\nĐúng với mô tả:đúng hàng\\nChất li...   \n"," 2213  Đúng với mô tả:đẹp\\nChất liệu:vải nỉ hơi xót n...   \n"," 2214  Đúng với mô tả:k đúng lắm\\nMàu sắc:nâu\\nChất l...   \n"," 2215                             đẹp nhưng vải hơi mỏng   \n"," 2216  Đã nhận được hàng theo đơn. From áo hơi to, áo...   \n"," \n","                                               clean_cmt in - out  star  \n"," 0                                           bình_thường       in     3  \n"," 1     mỏng phù_hợp giá khi đã được sale còn 55 nghìn...       in     4  \n"," 2     chất_liệu vải không đẹp bóng bóng đúng với mô_...       in     3  \n"," 3                        áo bẩn quá coi như hỏng cái áo       in     1  \n"," 4     mua lúc đang sale 29 nghìn thì thất_vọng quá v...       in     1  \n"," ...                                                 ...      ...   ...  \n"," 2212  màu sắc đen đúng với mô_tả đúng hàng chất_liệu...       in     4  \n"," 2213  đúng với mô_tả đẹp chất_liệu vải nỉ hơi xót nh...       in     4  \n"," 2214  đúng với mô_tả không đúng lắm màu_sắc nâu chất...       in     4  \n"," 2215                             đẹp nhưng vải hơi mỏng       in     4  \n"," 2216  đã nhận được hàng theo đơn form áo hơi to áo k...       in     4  \n"," \n"," [2217 rows x 8 columns],\n"," 'Tâm':       rating_star        cmtid       itemid  \\\n"," 0               4  13006790890  20592989947   \n"," 1               4  12850044341  17778903479   \n"," 2               4  13328343015  21693577588   \n"," 3               4   3843471062   1592182206   \n"," 4               4  12854843854  16694691238   \n"," ...           ...          ...          ...   \n"," 2212            5  13046110955  18894521842   \n"," 2213            5  13185639282  23970387156   \n"," 2214            5  13051707142  23208278966   \n"," 2215            5  12672561784  14395790114   \n"," 2216            5  13256054774  22582269266   \n"," \n","                                                    name  \\\n"," 0     Áo thun nữ , áo kiểu đẹp .Chất thun len tăm mề...   \n"," 1     Áo Khoác Gió Nữ 2 Lớp Cao Cấp Hidoli,Áo Gió Nữ...   \n"," 2     [RẺ VÔ ĐỊCH] Áo Nỉ Sweater Chất Nỉ Cotton In N...   \n"," 3                     Áo len tăm cổ 3 phân (quảng châu)   \n"," 4     ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n"," ...                                                 ...   \n"," 2212  Áo khoác nỉ croptop LEMONE dáng rộng siêu xinh...   \n"," 2213  Quần jeans ống loe màu bạc lưng cao co giãn ha...   \n"," 2214  Áo nỉ nữ polo khóa cổ dài tay form rộng in...   \n"," 2215  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 2216  Áo Nỉ Sweater Nữ Form Rộng Vải Nỉ Bông Dày Dặn...   \n"," \n","                                                 comment  \\\n"," 0     Chất liệu:dẹp\\nĐúng với mô tả:đúng\\nMàu sắc:xá...   \n"," 1     Màu sắc:xanh nhạt, đen\\n\\nÁo cũng được nhưng h...   \n"," 2     Màu sắc:xám\\nChất liệu:hơi mỏng\\nĐúng với mô t...   \n"," 3     Áo len vải đẹp, nhưng có nhiều vải thừa và mìn...   \n"," 4     Áo vẫn đẹo như ngày nào vải đẹp nữa nhất định ...   \n"," ...                                                 ...   \n"," 2212  Ship nhanh,vải mềm đẹp nên đã mua 4 màu. Trước...   \n"," 2213  Đúng với mô tả:tr ơi xinh lắm lun\\nMàu sắc:bạc...   \n"," 2214  Đúng với mô tả:đúng\\nMàu sắc:be sáng\\nChất liệ...   \n"," 2215  Đúng với mô tả:ddunsg\\nChất liệu:len\\nMàu sắc:...   \n"," 2216  Chất liệu:nỉ\\nĐúng với mô tả:đúng\\nMàu sắc:ghi...   \n"," \n","                                               clean_cmt in - out  star  \n"," 0     chất_liệu dẹp đúng với mô_tả đúng màu_sắc xám ...       in     5  \n"," 1     màu_sắc xanh nhạt đen áo cũng được nhưng hơi n...       in     2  \n"," 2     màu_sắc xám chất_liệu hơi mỏng đúng với mô_tả ...       in     3  \n"," 3     áo len vải đẹp nhưng có nhiều vải thừa và mình...       in     3  \n"," 4     áo vẫn đẹo như ngày nào vải đẹp nữa nhất_định ...       in     5  \n"," ...                                                 ...      ...   ...  \n"," 2212  ship nhanh vải mềm đẹp nên đã mua bốn màu trướ...      out     0  \n"," 2213  đúng với mô_tả trời_ơi xinh lắm luôn màu_sắc b...       in     5  \n"," 2214  đúng với mô_tả đúng màu_sắc be sáng chất_liệu ...       in     5  \n"," 2215  đúng với mô_tả ddunsg chất_liệu len màu_sắc hồ...       in     5  \n"," 2216  chất_liệu nỉ đúng với mô_tả đúng màu_sắc ghi đ...       in     5  \n"," \n"," [2217 rows x 8 columns],\n"," 'Thư':       rating_star        cmtid       itemid  \\\n"," 0               5  13071032325  22582269266   \n"," 1               5  13241788266  23265499251   \n"," 2               5  12855552143  23208278966   \n"," 3               5  13230952639  19082573960   \n"," 4               5  13200938967  23259871114   \n"," ...           ...          ...          ...   \n"," 2212            5  12670159867  14395790114   \n"," 2213            5  13012128541  22148769772   \n"," 2214            5  13266910241  23265499251   \n"," 2215            5   7232432748   1592182206   \n"," 2216            5  13176884157  23208278966   \n"," \n","                                                    name  \\\n"," 0     Áo Nỉ Sweater Nữ Form Rộng Vải Nỉ Bông Dày Dặn...   \n"," 1                Len Milk Cotton 50gr (từ mã 01 đến 50)   \n"," 2     Áo nỉ nữ polo khóa cổ dài tay form rộng in...   \n"," 3     Áo Khoác Hoodie Zip Chất Nỉ Bông Mũ 2 Lớp In C...   \n"," 4     Áo Len Tay Dài Cổ Tròn Dáng Dài From Rộng Phối...   \n"," ...                                                 ...   \n"," 2212  DIDIKA Áo Len Tay Dài Cổ Tròn Dáng Rộng Họa Ti...   \n"," 2213  Áo thun local brand ONTOP Ripple, Áo thun Unis...   \n"," 2214             Len Milk Cotton 50gr (từ mã 01 đến 50)   \n"," 2215                  Áo len tăm cổ 3 phân (quảng châu)   \n"," 2216  Áo nỉ nữ polo khóa cổ dài tay form rộng in...   \n"," \n","                                                 comment  \\\n"," 0     Màu sắc:xám\\nĐúng với mô tả:chuẩn\\nChất liệu:v...   \n"," 1     Đúng với mô tả:đúng\\nMàu sắc:đúng\\nChất liệu:đ...   \n"," 2     Màu sắc:đen\\nChất liệu:vải\\n\\nÁo đẹp nha vải h...   \n"," 3     Màu sắc:đen\\nChất liệu:vải nỉ\\nĐúng với mô tả:...   \n"," 4     Màu sắc:ok\\nChất liệu:ok\\nĐúng với mô tả:ok\\n\\...   \n"," ...                                                 ...   \n"," 2212  Đúng với mô tả:đúng\\nMàu sắc:xanh\\nChất liệu:l...   \n"," 2213  Đúng với mô tả:Đúng\\nMàu sắc:đen\\nChất liệu:th...   \n"," 2214  Shop bán rẻ và đẹp lắm luon giao hàng còn nhân...   \n"," 2215  Áo đẹp lắm luôn vải mềm co dãn tốt dày dặn mua...   \n"," 2216  Chất liệu:nỉ hơi mỏng\\nMàu sắc:nâu phối kem\\nĐ...   \n"," \n","                                               clean_cmt in - out  star  \n"," 0     màu_sắc xám đúng với mô_tả chuẩn chất_liệu vải...       in     4  \n"," 1     đúng với mô_tả đúng màu_sắc đúng chất_liệu đún...       in     4  \n"," 2     màu sắc đen chất_liệu vải áo đẹp nha vải hơi m...       in     3  \n"," 3     màu sắc đen chất_liệu vải nỉ đúng với mô_tả đú...       in     5  \n"," 4     màu_sắc ok chất_liệu ok đúng với mô_tả ok thun...       in     3  \n"," ...                                                 ...      ...   ...  \n"," 2212  đúng với mô_tả đúng màu_sắc xanh chất_liệu len...       in     5  \n"," 2213  đúng với mô_tả đúng màu sắc đen chất_liệu thun...       in     5  \n"," 2214  shop bán rẻ và đẹp lắm luon giao hàng còn nhân...       in     5  \n"," 2215  áo đẹp lắm luôn vải mềm co_dãn tốt dày_dặn mua...       in     5  \n"," 2216  chất_liệu nỉ hơi mỏng màu_sắc nâu phối kem đún...       in     3  \n"," \n"," [2217 rows x 8 columns],\n"," 'Thuy':       rating_star        cmtid       itemid  \\\n"," 0               5   6277628128   1592182206   \n"," 1               5  12769052716  18691602975   \n"," 2               5  13148330242  17695861884   \n"," 3               5  12652600029  20682429404   \n"," 4               5  13285305871  22148769772   \n"," ...           ...          ...          ...   \n"," 2213            1  12198892833  22313943551   \n"," 2214            2  13257460773  19093790478   \n"," 2215            1  13061370615   7141012796   \n"," 2216            5  13194167688  21695044273   \n"," 2217            4  10421157301   6888422353   \n"," \n","                                                    name  \\\n"," 0                     Áo len tăm cổ 3 phân (quảng châu)   \n"," 1     Quần Ống Rộng Nữ Cạp Chun Vải Đũi Cao Cấp Lên ...   \n"," 2     Áo khoác Nỉ Crotop Hàn dáng rộng siêu xinh , Á...   \n"," 3     Quần Jean Nữ Ống Loe To Co giãn Lưng cao,Quần ...   \n"," 4     Áo thun local brand ONTOP Ripple, Áo thun Unis...   \n"," ...                                                 ...   \n"," 2213  💛 Loại Đẹp 💛 Tất Nam Nữ Cổ Cao Cổ Ngắn Thể Tha...   \n"," 2214  Áo khoác nỉ croptop Keith House dáng rộng siêu...   \n"," 2215  Áo Thun Cổ Tròn Lệch Vai Xoắn Eo Tôn Dáng Nhiề...   \n"," 2216  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n"," 2217  Quần Ống Rộng Cullotes Vải Nữ Cạp Cao Dài Suôn...   \n"," \n","                                                 comment  \\\n"," 0     Áo mềm mặc ôm người. Mặc không bị bí đâu ạ. Đâ...   \n"," 1     Màu sắc:den\\nĐúng với mô tả:dung\\nChất liệu:vả...   \n"," 2     Màu sắc:trắng be\\nĐúng với mô tả:đúng\\nChất li...   \n"," 3     Quần bò màu xinh chất liệu mềm mại co giãn nhẹ...   \n"," 4     Chất liệu:vải\\nMàu sắc:đen\\nĐúng với mô tả:đún...   \n"," ...                                                 ...   \n"," 2213                                 24k 1 đôi mẹ ơi=))   \n"," 2214                           150k cái áo chất quá xấu   \n"," 2215  🥲 ảnh màu đen mà nhận màu xanh than,fom áo đẹp...   \n"," 2216  😎Hàng đóng gói rất đẹp. Quả nhiên người bán đã...   \n"," 2217  🔥🔥🔥Tặng 100 Khách Giá Gốc Hôm Nay!!!\\n.ad báo ...   \n"," \n","                                               clean_cmt in - out  star  \n"," 0     áo mềm mặc ôm người mặc không bị bí đâu ạ đây ...       in   5.0  \n"," 1     màu_sắc den đúng với mô_tả dung chất_liệu vải ...       in   4.0  \n"," 2     màu_sắc trắng be đúng với mô_tả đúng chất_liệu...       in   5.0  \n"," 3     quần_bò màu xinh chất_liệu mềm_mại co_giãn nhẹ...       in   5.0  \n"," 4     chất_liệu vải màu sắc đen đúng với mô_tả đúng ...       in   5.0  \n"," ...                                                 ...      ...   ...  \n"," 2213                             24 nghìn một đôi mẹ ơi      out   0.0  \n"," 2214                      150 nghìn cái áo chất quá xấu       in   1.0  \n"," 2215  <emoji> smiling face with tear </emoji> ảnh mà...       in   3.0  \n"," 2216  <emoji> smiling face with sunglasses </emoji> ...       in   5.0  \n"," 2217  <emoji> fire </emoji> <emoji> fire </emoji> <e...      out   0.0  \n"," \n"," [2218 rows x 8 columns]}"]},"metadata":{},"execution_count":7}],"source":["df_remain = pd.read_excel(path.join(annotated_path, 'annotated_data.xlsx'), engine=\"openpyxl\", sheet_name = None)\n","df_remain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1714956138912,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"u4TK2jJvQGtu","outputId":"b0b5df56-2431-4df4-e0c0-73eba11aa89a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      rating_star        cmtid       itemid  \\\n","0               5  12486152255  23676957049   \n","1               5  12999867370  23754591984   \n","2               4  12360238836  23676957049   \n","3               5  12877630757  23182460672   \n","4               1  12154882262  16694691238   \n","...           ...          ...          ...   \n","2213            1  12198892833  22313943551   \n","2214            2  13257460773  19093790478   \n","2215            1  13061370615   7141012796   \n","2216            5  13194167688  21695044273   \n","2217            4  10421157301   6888422353   \n","\n","                                                   name  \\\n","0     áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n","1     Quần jean ống suông rách gối , Quần bò dáng su...   \n","2     áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n","3     Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n","4     ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n","...                                                 ...   \n","2213  💛 Loại Đẹp 💛 Tất Nam Nữ Cổ Cao Cổ Ngắn Thể Tha...   \n","2214  Áo khoác nỉ croptop Keith House dáng rộng siêu...   \n","2215  Áo Thun Cổ Tròn Lệch Vai Xoắn Eo Tôn Dáng Nhiề...   \n","2216  Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...   \n","2217  Quần Ống Rộng Cullotes Vải Nữ Cạp Cao Dài Suôn...   \n","\n","                                                comment  \\\n","0     Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n","1     Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n","2     Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n","3     \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n","4                                 Xấu tốt nhất đừng mua   \n","...                                                 ...   \n","2213                                 24k 1 đôi mẹ ơi=))   \n","2214                           150k cái áo chất quá xấu   \n","2215  🥲 ảnh màu đen mà nhận màu xanh than,fom áo đẹp...   \n","2216  😎Hàng đóng gói rất đẹp. Quả nhiên người bán đã...   \n","2217  🔥🔥🔥Tặng 100 Khách Giá Gốc Hôm Nay!!!\\n.ad báo ...   \n","\n","                                              clean_cmt in - out star  \n","0     xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in  5.0  \n","1     xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in  5.0  \n","2     xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in  5.0  \n","3     xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in  5.0  \n","4                                 xấu tốt nhất đừng mua       in  1.0  \n","...                                                 ...      ...  ...  \n","2213                             24 nghìn một đôi mẹ ơi      out  0.0  \n","2214                      150 nghìn cái áo chất quá xấu       in  1.0  \n","2215  <emoji> smiling face with tear </emoji> ảnh mà...       in  3.0  \n","2216  <emoji> smiling face with sunglasses </emoji> ...       in  5.0  \n","2217  <emoji> fire </emoji> <emoji> fire </emoji> <e...      out  0.0  \n","\n","[14303 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-30129a41-ceef-4ef8-bdd4-6d39ab7320e7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rating_star</th>\n","      <th>cmtid</th>\n","      <th>itemid</th>\n","      <th>name</th>\n","      <th>comment</th>\n","      <th>clean_cmt</th>\n","      <th>in - out</th>\n","      <th>star</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>12486152255</td>\n","      <td>23676957049</td>\n","      <td>áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...</td>\n","      <td>Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...</td>\n","      <td>xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5</td>\n","      <td>12999867370</td>\n","      <td>23754591984</td>\n","      <td>Quần jean ống suông rách gối , Quần bò dáng su...</td>\n","      <td>Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...</td>\n","      <td>xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>12360238836</td>\n","      <td>23676957049</td>\n","      <td>áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...</td>\n","      <td>Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...</td>\n","      <td>xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>12877630757</td>\n","      <td>23182460672</td>\n","      <td>Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...</td>\n","      <td>\"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....</td>\n","      <td>xinh lắm luôn á mọi người ơi chất vải mềm lêm ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>12154882262</td>\n","      <td>16694691238</td>\n","      <td>ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...</td>\n","      <td>Xấu tốt nhất đừng mua</td>\n","      <td>xấu tốt nhất đừng mua</td>\n","      <td>in</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2213</th>\n","      <td>1</td>\n","      <td>12198892833</td>\n","      <td>22313943551</td>\n","      <td>💛 Loại Đẹp 💛 Tất Nam Nữ Cổ Cao Cổ Ngắn Thể Tha...</td>\n","      <td>24k 1 đôi mẹ ơi=))</td>\n","      <td>24 nghìn một đôi mẹ ơi</td>\n","      <td>out</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2214</th>\n","      <td>2</td>\n","      <td>13257460773</td>\n","      <td>19093790478</td>\n","      <td>Áo khoác nỉ croptop Keith House dáng rộng siêu...</td>\n","      <td>150k cái áo chất quá xấu</td>\n","      <td>150 nghìn cái áo chất quá xấu</td>\n","      <td>in</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2215</th>\n","      <td>1</td>\n","      <td>13061370615</td>\n","      <td>7141012796</td>\n","      <td>Áo Thun Cổ Tròn Lệch Vai Xoắn Eo Tôn Dáng Nhiề...</td>\n","      <td>🥲 ảnh màu đen mà nhận màu xanh than,fom áo đẹp...</td>\n","      <td>&lt;emoji&gt; smiling face with tear &lt;/emoji&gt; ảnh mà...</td>\n","      <td>in</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2216</th>\n","      <td>5</td>\n","      <td>13194167688</td>\n","      <td>21695044273</td>\n","      <td>Áo gile len vặn thừng nữ cổ V dáng rộng DELIZ ...</td>\n","      <td>😎Hàng đóng gói rất đẹp. Quả nhiên người bán đã...</td>\n","      <td>&lt;emoji&gt; smiling face with sunglasses &lt;/emoji&gt; ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>2217</th>\n","      <td>4</td>\n","      <td>10421157301</td>\n","      <td>6888422353</td>\n","      <td>Quần Ống Rộng Cullotes Vải Nữ Cạp Cao Dài Suôn...</td>\n","      <td>🔥🔥🔥Tặng 100 Khách Giá Gốc Hôm Nay!!!\\n.ad báo ...</td>\n","      <td>&lt;emoji&gt; fire &lt;/emoji&gt; &lt;emoji&gt; fire &lt;/emoji&gt; &lt;e...</td>\n","      <td>out</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14303 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30129a41-ceef-4ef8-bdd4-6d39ab7320e7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-30129a41-ceef-4ef8-bdd4-6d39ab7320e7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-30129a41-ceef-4ef8-bdd4-6d39ab7320e7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-05267080-5fe0-4158-823c-77f7ef52db79\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05267080-5fe0-4158-823c-77f7ef52db79')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-05267080-5fe0-4158-823c-77f7ef52db79 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 14303,\n  \"fields\": [\n    {\n      \"column\": \"rating_star\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cmtid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1426496512,\n        \"min\": 1015612654,\n        \"max\": 13347066052,\n        \"num_unique_values\": 14303,\n        \"samples\": [\n          13131355928,\n          13091599652,\n          10814177612\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5196743013,\n        \"min\": 1592182206,\n        \"max\": 25450823380,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          19692270976,\n          23970387156,\n          25450823380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"\\u00e1o kho\\u00e1c hoodie zip tay d\\u00e0i fom r\\u1ed9ng c\\u1ef1c \\u0111\\u00e1ng iu d\\u1ec5 th\\u01b0\\u01a1ng , \\u00c1o Hoodie nam n\\u1eef ph\\u1ed1i xanh than T63\",\n          \"Qu\\u1ea7n jeans \\u1ed1ng loe m\\u00e0u b\\u1ea1c l\\u01b0ng cao co gi\\u00e3n hack d\\u00e1ng\",\n          \"XMCU07 S\\u00e9t Nhung QC Ph\\u1ed1i Vi\\u1ec1n, S\\u00e9t B\\u1ed9 Ng\\u1ee7 Cho N\\u1eef Sang Ch\\u1ea3nh Nh\\u1eb9 Nh\\u00e0ng Theo Phong C\\u00e1ch H\\u00e0n Qu\\u1ed1c - X\\u01b0\\u1edfng May Chi\\u1ebfn \\u00dat\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14302,\n        \"samples\": [\n          \"\\u0110\\u00f3ng g\\u00f3i s\\u1ea3n ph\\u1ea9m c\\u1ea9n th\\u1eadn,s\\u1ea3n ph\\u1ea9m r\\u1ea5t \\u0111\\u1eb9p mua c\\u1ee7a shop 2 l\\u1ea7n r\\u1ed3i h\\u00e0i l\\u00f2ng nh\\u00e9 m\\u1ecdi ng\\u01b0\\u1eddi\",\n          \"qu\\u1ea7n m\\u1ecfng n\\u1ebfu m\\u1eb7c \\u00e1o thun d\\u00e0i s\\u1ebd kh l\\u1ed9 c\\u00f2n m\\u00e0u th\\u00ec l\\u00ean \\u1ea3nh kh gi\\u1ed1ng qu\\u1ea7n \\u1edf ngo\\u00e0i m\\u00e0u \\u0111\\u1eadm h\\u01a1n tui th\\u1ea5y h\\u01a1i v\\u00e0ng v\\u00e0ng\",\n          \"Ch\\u1ea5t li\\u1ec7u:m\\u1ecfng\\nM\\u00e0u s\\u1eafc:t\\u1ea1m\\n\\nTay \\u00e1o  ko \\u0111\\u00fang v\\u1edbi tr\\u00ean \\u1ea3nh r\\u1ed9ng n\\u00f3i chung l\\u00e0 cheeeee\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_cmt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14278,\n        \"samples\": [\n          \"14 ng\\u00e0y m\\u1edbi giao \\u0111\\u1ebfn nh\\u01b0ng xinh x\\u1ec9u lu\\u00f4n \\u00e1 <emoji> smiling face with hearts </emoji> <emoji> smiling face with hearts </emoji> m\\u00ecnh m6 40 kilogram m\\u1eb7c r\\u1ed9ng tho\\u1ea3i_m\\u00e1i nhe len m\\u1ecfng n\\u00ean m\\u1ecdi ng\\u01b0\\u1eddi suy_ngh\\u0129 mua nh\\u00e9 nh\\u01b0ng c\\u0169ng kh\\u00e1 \\u1ea5m <emoji> kissing face </emoji>\",\n          \"\\u0111\\u00fang v\\u1edbi m\\u00f4_t\\u1ea3 h\\u01a1i m\\u1ecfng ch\\u1ea5t_li\\u1ec7u t\\u00e2m m\\u00e0u_s\\u1eafc be\",\n          \"\\u00e1o \\u1ed5n so v\\u1edbi gi\\u00e1 ch\\u1ea5t gi\\u1ed1ng v\\u1edbi nh\\u1eefng \\u00e1o kh\\u00e1c m\\u00ecnh t\\u1eebng mua m\\u1ed9t m\\u00e9t 67 m\\u1eb7c d\\u00e0i g\\u1ea7n qua m\\u00f4ng c\\u00f3_\\u0111i\\u1ec1u <emoji> cross mark </emoji> tay_\\u00e1o si\\u00eau ng\\u1eafn <emoji> cross mark </emoji> c\\u00e1c b\\u1ea1n th\\u00edch tay d\\u00e0i n\\u00ean c\\u00e2n_nh\\u1eafc nha m\\u00ecnh kh\\u00e1 th\\u1ea5t_v\\u1ecdng v\\u1ec1 ph\\u1ea7n tay\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in - out\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"out\",\n          \"in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"star\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":8}],"source":["df_name = []\n","for k, v in df_remain.items():\n","  if k == 'src':\n","    continue\n","  else:\n","    df_name.append(v)\n","\n","df = pd.concat(df_name)\n","df = pd.concat([clean_df, df])\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":7780,"status":"ok","timestamp":1714956146670,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"oyvrm7XRKD4h","outputId":"57f851f2-7aa9-4321-d8af-1c92cfae27f1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       rating_star        cmtid       itemid  \\\n","0                5  12486152255  23676957049   \n","1                5  12999867370  23754591984   \n","2                4  12360238836  23676957049   \n","3                5  12877630757  23182460672   \n","4                1  12154882262  16694691238   \n","...            ...          ...          ...   \n","14297            1  12198892833  22313943551   \n","14298            2  13257460773  19093790478   \n","14299            1  13061370615   7141012796   \n","14300            5  13194167688  21695044273   \n","14301            4  10421157301   6888422353   \n","\n","                                                    name  \\\n","0       Sơ mi trễ vai tay ngắn, sơ mi nữ kiểu dáng di...   \n","1       Quần jeans rách ống suông, quần jeans rách ốn...   \n","2       Sơ mi trễ vai tay ngắn, sơ mi nữ kiểu dáng di...   \n","3       Áo thun nữ dài tay dây rút 2 bên, cổ tròn, fo...   \n","4       ÁO THUN UNISEX/ÁO THUN MẪU CHÂU ÂU RAGE OF TH...   \n","...                                                  ...   \n","14297   💛 Loại Đẹp 💛 Tất thể thao cổ cao ngắn nam nữ ...   \n","14298   Keith House siêu dễ thương dáng rộng phong cá...   \n","14299         Áo thun cổ tròn lệch vai nhiều màu Ulzzang   \n","14300   Áo vest len cổ chữ V dáng rộng dành cho nữ Áo...   \n","14301   Quần Cullotes vải nữ Quần ống rộng lưng cao Q...   \n","\n","                                                 comment  \\\n","0       Tuyệt vời, giao hàng nhanh, giá rẻ, đóng gói ...   \n","1       Đẹp quá, màu sắc thật tuyệt vời. Còn ngần ngạ...   \n","2       Đẹp đấy nhưng hơi rộng nên mình phải để ở phí...   \n","3       \"Xinh lắm mọi người. Chất vải mềm mịn, form d...   \n","4                         Xấu, tốt nhất là không nên mua   \n","...                                                  ...   \n","14297                               24k 1 đôi nhé mẹ =))   \n","14298                                150k áo chất quá tệ   \n","14299  🥲Hình màu đen nhưng màu xanh than. Form áo đẹp...   \n","14300   😎Bao bì rất đẹp. Đúng như dự đoán, người bán ...   \n","14301  🔥🔥🔥Tặng 100 khách hàng giá gốc ngay hôm nay!!!...   \n","\n","                                               clean_cmt in - out star  \n","0      tuyệt_vời giao hàng nhanh giá rẻ đóng_gói cẩn_...       in    5  \n","1      đẹp quá màu_sắc thật tuyệt_vời còn ngần_ngại t...       in    5  \n","2      đẹp đấy nhưng hơi rộng nên mình phải để ở phía...       in    5  \n","3      xinh lắm mọi người chất vải mềm mịn form dáng ...       in    5  \n","4                          xấu tốt nhất là không nên mua       in    1  \n","...                                                  ...      ...  ...  \n","14297                            24 nghìn một đôi nhé mẹ      out    0  \n","14298                           150 nghìn áo chất quá tệ       in    1  \n","14299  <emoji> smiling face with tear </emoji> hình m...       in    3  \n","14300  <emoji> smiling face with sunglasses </emoji> ...       in    5  \n","14301  <emoji> fire </emoji> <emoji> fire </emoji> <e...      out    0  \n","\n","[14302 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-07950e08-86c8-46c4-9a22-afd9cf11eda6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rating_star</th>\n","      <th>cmtid</th>\n","      <th>itemid</th>\n","      <th>name</th>\n","      <th>comment</th>\n","      <th>clean_cmt</th>\n","      <th>in - out</th>\n","      <th>star</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>12486152255</td>\n","      <td>23676957049</td>\n","      <td>Sơ mi trễ vai tay ngắn, sơ mi nữ kiểu dáng di...</td>\n","      <td>Tuyệt vời, giao hàng nhanh, giá rẻ, đóng gói ...</td>\n","      <td>tuyệt_vời giao hàng nhanh giá rẻ đóng_gói cẩn_...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5</td>\n","      <td>12999867370</td>\n","      <td>23754591984</td>\n","      <td>Quần jeans rách ống suông, quần jeans rách ốn...</td>\n","      <td>Đẹp quá, màu sắc thật tuyệt vời. Còn ngần ngạ...</td>\n","      <td>đẹp quá màu_sắc thật tuyệt_vời còn ngần_ngại t...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>12360238836</td>\n","      <td>23676957049</td>\n","      <td>Sơ mi trễ vai tay ngắn, sơ mi nữ kiểu dáng di...</td>\n","      <td>Đẹp đấy nhưng hơi rộng nên mình phải để ở phí...</td>\n","      <td>đẹp đấy nhưng hơi rộng nên mình phải để ở phía...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>12877630757</td>\n","      <td>23182460672</td>\n","      <td>Áo thun nữ dài tay dây rút 2 bên, cổ tròn, fo...</td>\n","      <td>\"Xinh lắm mọi người. Chất vải mềm mịn, form d...</td>\n","      <td>xinh lắm mọi người chất vải mềm mịn form dáng ...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>12154882262</td>\n","      <td>16694691238</td>\n","      <td>ÁO THUN UNISEX/ÁO THUN MẪU CHÂU ÂU RAGE OF TH...</td>\n","      <td>Xấu, tốt nhất là không nên mua</td>\n","      <td>xấu tốt nhất là không nên mua</td>\n","      <td>in</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14297</th>\n","      <td>1</td>\n","      <td>12198892833</td>\n","      <td>22313943551</td>\n","      <td>💛 Loại Đẹp 💛 Tất thể thao cổ cao ngắn nam nữ ...</td>\n","      <td>24k 1 đôi nhé mẹ =))</td>\n","      <td>24 nghìn một đôi nhé mẹ</td>\n","      <td>out</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14298</th>\n","      <td>2</td>\n","      <td>13257460773</td>\n","      <td>19093790478</td>\n","      <td>Keith House siêu dễ thương dáng rộng phong cá...</td>\n","      <td>150k áo chất quá tệ</td>\n","      <td>150 nghìn áo chất quá tệ</td>\n","      <td>in</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14299</th>\n","      <td>1</td>\n","      <td>13061370615</td>\n","      <td>7141012796</td>\n","      <td>Áo thun cổ tròn lệch vai nhiều màu Ulzzang</td>\n","      <td>🥲Hình màu đen nhưng màu xanh than. Form áo đẹp...</td>\n","      <td>&lt;emoji&gt; smiling face with tear &lt;/emoji&gt; hình m...</td>\n","      <td>in</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>14300</th>\n","      <td>5</td>\n","      <td>13194167688</td>\n","      <td>21695044273</td>\n","      <td>Áo vest len cổ chữ V dáng rộng dành cho nữ Áo...</td>\n","      <td>😎Bao bì rất đẹp. Đúng như dự đoán, người bán ...</td>\n","      <td>&lt;emoji&gt; smiling face with sunglasses &lt;/emoji&gt; ...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>14301</th>\n","      <td>4</td>\n","      <td>10421157301</td>\n","      <td>6888422353</td>\n","      <td>Quần Cullotes vải nữ Quần ống rộng lưng cao Q...</td>\n","      <td>🔥🔥🔥Tặng 100 khách hàng giá gốc ngay hôm nay!!!...</td>\n","      <td>&lt;emoji&gt; fire &lt;/emoji&gt; &lt;emoji&gt; fire &lt;/emoji&gt; &lt;e...</td>\n","      <td>out</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14302 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07950e08-86c8-46c4-9a22-afd9cf11eda6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-07950e08-86c8-46c4-9a22-afd9cf11eda6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-07950e08-86c8-46c4-9a22-afd9cf11eda6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-f53ec0bb-da81-4acd-a542-e447ebda0149\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f53ec0bb-da81-4acd-a542-e447ebda0149')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-f53ec0bb-da81-4acd-a542-e447ebda0149 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_extra","summary":"{\n  \"name\": \"df_extra\",\n  \"rows\": 14302,\n  \"fields\": [\n    {\n      \"column\": \"rating_star\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cmtid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1423986239,\n        \"min\": 1015612654,\n        \"max\": 13347066052,\n        \"num_unique_values\": 14302,\n        \"samples\": [\n          12902256662,\n          12190030614,\n          10814177612\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5195040587,\n        \"min\": 1592182206,\n        \"max\": 25450823380,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          19692270976,\n          23970387156,\n          25450823380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 192,\n        \"samples\": [\n          \"\\ud83d\\udc9b Lo\\u1ea1i \\u0110\\u1eb9p \\ud83d\\udc9b T\\u1ea5t th\\u1ec3 thao c\\u1ed5 cao ng\\u1eafn nam n\\u1eef th\\u1eddi trang cao c\\u1ea5p - T\\u1ea5t nam n\\u1eef Nike V\\u1ea3i d\\u1ec7t kim kh\\u1eed m\\u00f9i kh\\u00e1ng khu\\u1ea9n\",\n          \" \\u00c1o kho\\u00e1c n\\u1eef, \\u00c1o kho\\u00e1c l\\u00f4ng c\\u1eebu k\\u1ebb s\\u1ecdc, S\\u1ea3n ph\\u1ea9m QCCC\",\n          \" \\u00c1o len d\\u00e0i tay c\\u1ed5 ch\\u1eef O d\\u00e1ng r\\u1ed9ng, h\\u1ecda ti\\u1ebft m\\u00e0u s\\u1eafc t\\u01b0\\u01a1ng ph\\u1ea3n, d\\u1ec5 ph\\u1ed1i \\u0111\\u1ed3 Phong c\\u00e1ch H\\u00e0n Qu\\u1ed1c Hot Model 2024 Orang\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14262,\n        \"samples\": [\n          \"\\u00c1o \\u0111\\u1eb9p l\\u1eafm, so v\\u1edbi gi\\u00e1 th\\u00ec qu\\u00e1 \\u0111\\u1eb9p. Shop giao h\\u00e0ng c\\u1ef1c k\\u00ec nhanh. \\u0110\\u1eb7t t\\u1ed1i qua m\\u00e0 chi\\u1ec1u h\\u00f4m sau \\u0111\\u00e3 nh\\u1eadn \\u0111\\u01b0\\u1ee3c. \\u2764\\ufe0f\\ud83d\\udda4\\ud83d\\udda4\\ud83d\\udda4\\ud83d\\udc9c\\ud83d\\udc9c\\ud83d\\udc9c\",\n          \" \\u00c1o r\\u1ea5t d\\u00e0y, v\\u1eeba v\\u1eb7n v\\u00e0 nh\\u00ecn \\u0111\\u1eb9p, gi\\u00e1 c\\u1ea3 ph\\u1ea3i ch\\u0103ng v\\u00e0 giao h\\u00e0ng nhanh ch\\u00f3ng\",\n          \" Giao h\\u00e0ng r\\u1ea5t nhanh, ch\\u1ea5t l\\u01b0\\u1ee3ng \\u00e1o c\\u0169ng \\u0111\\u1eb9p. M\\u00ecnh s\\u1ebd \\u1ee7ng h\\u1ed9 shoppp l\\u00e2u d\\u00e0i\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_cmt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14232,\n        \"samples\": [\n          \"ch\\u1ea5t v\\u1ea3i x\\u1ea5u th\\u00f4 c\\u00f3 v\\u1ebft \\u0111en nh\\u01b0 m\\u1ed1c\",\n          \"ch\\u1ea5t_l\\u01b0\\u1ee3ng s\\u1ea3n_ph\\u1ea9m r\\u1ea5t t\\u1ed1t giao h\\u00e0ng c\\u0169ng kh\\u00e1 nhanh gi\\u00e1 r\\u1ebb ch\\u00fac c\\u1eeda_h\\u00e0ng may_m\\u1eafn\",\n          \"m\\u00e0u_s\\u1eafc x\\u00e1m \\u0111\\u1eadm ch\\u1ea5t_li\\u1ec7u v\\u1ea3i \\u0111\\u00fang m\\u00f4_t\\u1ea3 \\u00e1o r\\u1ea5t r\\u1ed9ng ch\\u1ec9 c\\u00f3 m\\u1ed9t l\\u1edbp r\\u1ebb n\\u00ean nh\\u00ecn_chung l\\u00e0 ok chuy\\u1ec3n_ph\\u00e1t nhanh\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in - out\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"out\",\n          \"in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"star\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":9}],"source":["df_extra = pd.read_excel(path.join(annotated_path, 'extra_data.xlsx'))\n","df_extra = df_extra[['rating_star', 'cmtid', 'itemid', 'name', 'comment', 'token', 'in - out', 'star']]\n","df_extra.rename(columns={'token': 'clean_cmt'}, inplace = True)\n","df_extra"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1714956146670,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"9drSu1oyXHBg","outputId":"1d9d124c-5632-4a9d-c836-cd7bb6011b09"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       rating_star        cmtid       itemid  \\\n","0                5  12486152255  23676957049   \n","1                5  12999867370  23754591984   \n","2                4  12360238836  23676957049   \n","3                5  12877630757  23182460672   \n","4                1  12154882262  16694691238   \n","...            ...          ...          ...   \n","14297            1  12198892833  22313943551   \n","14298            2  13257460773  19093790478   \n","14299            1  13061370615   7141012796   \n","14300            5  13194167688  21695044273   \n","14301            4  10421157301   6888422353   \n","\n","                                                    name  \\\n","0      áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n","1      Quần jean ống suông rách gối , Quần bò dáng su...   \n","2      áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n","3      Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n","4      ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n","...                                                  ...   \n","14297   💛 Loại Đẹp 💛 Tất thể thao cổ cao ngắn nam nữ ...   \n","14298   Keith House siêu dễ thương dáng rộng phong cá...   \n","14299         Áo thun cổ tròn lệch vai nhiều màu Ulzzang   \n","14300   Áo vest len cổ chữ V dáng rộng dành cho nữ Áo...   \n","14301   Quần Cullotes vải nữ Quần ống rộng lưng cao Q...   \n","\n","                                                 comment  \\\n","0      Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...   \n","1      Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...   \n","2      Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...   \n","3      \"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....   \n","4                                  Xấu tốt nhất đừng mua   \n","...                                                  ...   \n","14297                               24k 1 đôi nhé mẹ =))   \n","14298                                150k áo chất quá tệ   \n","14299  🥲Hình màu đen nhưng màu xanh than. Form áo đẹp...   \n","14300   😎Bao bì rất đẹp. Đúng như dự đoán, người bán ...   \n","14301  🔥🔥🔥Tặng 100 khách hàng giá gốc ngay hôm nay!!!...   \n","\n","                                               clean_cmt in - out star  \n","0      xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in  5.0  \n","1      xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in  5.0  \n","2      xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in  5.0  \n","3      xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in  5.0  \n","4                                  xấu tốt nhất đừng mua       in  1.0  \n","...                                                  ...      ...  ...  \n","14297                            24 nghìn một đôi nhé mẹ      out    0  \n","14298                           150 nghìn áo chất quá tệ       in    1  \n","14299  <emoji> smiling face with tear </emoji> hình m...       in    3  \n","14300  <emoji> smiling face with sunglasses </emoji> ...       in    5  \n","14301  <emoji> fire </emoji> <emoji> fire </emoji> <e...      out    0  \n","\n","[28605 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-5a478937-8ac5-41ec-8b25-06bbf4cd9284\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rating_star</th>\n","      <th>cmtid</th>\n","      <th>itemid</th>\n","      <th>name</th>\n","      <th>comment</th>\n","      <th>clean_cmt</th>\n","      <th>in - out</th>\n","      <th>star</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>12486152255</td>\n","      <td>23676957049</td>\n","      <td>áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...</td>\n","      <td>Xuất sắc luôn , giao hàng nhanh, giá rẻ , đóng...</td>\n","      <td>xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5</td>\n","      <td>12999867370</td>\n","      <td>23754591984</td>\n","      <td>Quần jean ống suông rách gối , Quần bò dáng su...</td>\n","      <td>Xinh xỉu má ơiii cái màu đỉnh cựccc ai do dự t...</td>\n","      <td>xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>12360238836</td>\n","      <td>23676957049</td>\n","      <td>áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...</td>\n","      <td>Xinh nm hơi rộng huhu phải kẹp đằng sau nựa\\nR...</td>\n","      <td>xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>12877630757</td>\n","      <td>23182460672</td>\n","      <td>Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...</td>\n","      <td>\"Xinh lắm luôn á mọi người ơi. \\nChất vải mềm....</td>\n","      <td>xinh lắm luôn á mọi người ơi chất vải mềm lêm ...</td>\n","      <td>in</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>12154882262</td>\n","      <td>16694691238</td>\n","      <td>ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...</td>\n","      <td>Xấu tốt nhất đừng mua</td>\n","      <td>xấu tốt nhất đừng mua</td>\n","      <td>in</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14297</th>\n","      <td>1</td>\n","      <td>12198892833</td>\n","      <td>22313943551</td>\n","      <td>💛 Loại Đẹp 💛 Tất thể thao cổ cao ngắn nam nữ ...</td>\n","      <td>24k 1 đôi nhé mẹ =))</td>\n","      <td>24 nghìn một đôi nhé mẹ</td>\n","      <td>out</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14298</th>\n","      <td>2</td>\n","      <td>13257460773</td>\n","      <td>19093790478</td>\n","      <td>Keith House siêu dễ thương dáng rộng phong cá...</td>\n","      <td>150k áo chất quá tệ</td>\n","      <td>150 nghìn áo chất quá tệ</td>\n","      <td>in</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14299</th>\n","      <td>1</td>\n","      <td>13061370615</td>\n","      <td>7141012796</td>\n","      <td>Áo thun cổ tròn lệch vai nhiều màu Ulzzang</td>\n","      <td>🥲Hình màu đen nhưng màu xanh than. Form áo đẹp...</td>\n","      <td>&lt;emoji&gt; smiling face with tear &lt;/emoji&gt; hình m...</td>\n","      <td>in</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>14300</th>\n","      <td>5</td>\n","      <td>13194167688</td>\n","      <td>21695044273</td>\n","      <td>Áo vest len cổ chữ V dáng rộng dành cho nữ Áo...</td>\n","      <td>😎Bao bì rất đẹp. Đúng như dự đoán, người bán ...</td>\n","      <td>&lt;emoji&gt; smiling face with sunglasses &lt;/emoji&gt; ...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>14301</th>\n","      <td>4</td>\n","      <td>10421157301</td>\n","      <td>6888422353</td>\n","      <td>Quần Cullotes vải nữ Quần ống rộng lưng cao Q...</td>\n","      <td>🔥🔥🔥Tặng 100 khách hàng giá gốc ngay hôm nay!!!...</td>\n","      <td>&lt;emoji&gt; fire &lt;/emoji&gt; &lt;emoji&gt; fire &lt;/emoji&gt; &lt;e...</td>\n","      <td>out</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28605 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a478937-8ac5-41ec-8b25-06bbf4cd9284')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5a478937-8ac5-41ec-8b25-06bbf4cd9284 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5a478937-8ac5-41ec-8b25-06bbf4cd9284');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3d25d865-84ca-42df-957c-3c579fe4526a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3d25d865-84ca-42df-957c-3c579fe4526a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3d25d865-84ca-42df-957c-3c579fe4526a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 28605,\n  \"fields\": [\n    {\n      \"column\": \"rating_star\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cmtid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1425217103,\n        \"min\": 1015612654,\n        \"max\": 13347066052,\n        \"num_unique_values\": 14303,\n        \"samples\": [\n          13131355928,\n          13091599652,\n          10814177612\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5195801107,\n        \"min\": 1592182206,\n        \"max\": 25450823380,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          19692270976,\n          23970387156,\n          25450823380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 292,\n        \"samples\": [\n          \"\\u00c1o N\\u1ec9 Sweater ch\\u1eef IUS Ph\\u1ed1i Th\\u00eau Da H\\u1ed3ng , \\u00c1o N\\u1ec9 C\\u1ed5 Tr\\u00f2n M\\u00e0u X\\u00e1m Ti\\u00eau From R\\u1ed9ng Tay B\\u1ed3ng Unsiex M67\",\n          \"M\\u1eabu m\\u1edbi si\\u00eau hot \\u00c1o hoodie in ENG, \\u00e1o n\\u1ec9 d\\u00e1ng r\\u1ed9ng c\\u00f3 m\\u0169 tr\\u00f9m \\u0111\\u1ea7u 2 l\\u1edbp, d\\u00e2y r\\u00fat d\\u00e0y d\\u1eb7n\",\n          \"A\\u0301o N\\u1eef, \\u00c1o Hoodie \\u0110\\u00ednh V\\u1ea3i B\\u00f2 In Tho\\u0309 Ch\\u00e2\\u0301t Ni\\u0309 Ta\\u0300u Si\\u00eau Cute Hottrend 2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28561,\n        \"samples\": [\n          \"M\\u00e0u s\\u1eafc:be\\n\\u0110\\u00fang v\\u1edbi m\\u00f4 t\\u1ea3:like\\nCh\\u1ea5t li\\u1ec7u:v\\u1ea3i 1 l\\u1edbp , \\u0111\\u1eb9p\",\n          \" Ch\\u1ea5t li\\u1ec7u: kh\\u00f4ng gi\\u1ed1ng nhau M\\u00e0u s\\u1eafc: gi\\u1ed1ng nhau \\u0110\\u00fang m\\u00f4 t\\u1ea3: kh\\u00f4ng h\\u1ec1 Ch\\u1ea5t li\\u1ec7u v\\u1ea3i kh\\u00f4ng nh\\u01b0 t\\u00f4i mong \\u0111\\u1ee3i\",\n          \"Ch\\u1ea5t li\\u1ec7u:v\\u1ea3i n\\u00e0y d\\u00ednh t\\u00f3c\\nM\\u00e0u s\\u1eafc:be vi\\u1ec1n n\\u00e2u\\n\\u0110\\u00fang v\\u1edbi m\\u00f4 t\\u1ea3:ko m\\u1ecfng,l\\u00e0 lo\\u1ea1i v\\u1ea3i d\\u00ednh t\\u00f3c \\u00fd,mik ko bt t\\u00ean\\n\\n\\u00c1o cx ok \\u0111\\u00f3,ko m\\u1ecfng,ko d\\u00e0y, v\\u1eeba vs gi\\u00e1 ti\\u1ec1n.\\nMik cao 1m53,44kg mua size S ok, nh\\u01b0ng tay \\u00e1o d\\u00e0i 1 ch\\u00fat, nh\\u01b0ng ko sao .\\nT\\u00f3m l\\u1ea1i l\\u00e0 ok nha mn, \\u1ee7ng h\\u1ed9 shop nha mn\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_cmt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28288,\n        \"samples\": [\n          \"\\u0111\\u00fang m\\u00f4_t\\u1ea3 \\u0111\\u00fang ch\\u1ea5t_li\\u1ec7u jean m\\u00e0u_s\\u1eafc b\\u1ea1c qu\\u1ea7n \\u0111\\u1eb9p gi\\u1ed1ng h\\u00ecnh\",\n          \"m\\u00e0u_s\\u1eafc mua xanh nh\\u01b0ng giao m\\u00e0u \\u0111en \\u0111\\u00fang m\\u00f4_t\\u1ea3 x\\u1ea5u ch\\u1ea5t_li\\u1ec7u x\\u1ea5u qu\\u00e1 s\\u1ea3n_ph\\u1ea9m x\\u1ea5u qu\\u00e1 nh\\u01b0ng v\\u1eabn b\\u00e1n \\u0111\\u01b0\\u1ee3c\",\n          \"m\\u1eb9 \\u01a1i m\\u00e0u qu\\u1ea7n x\\u1ea5u qu\\u00e1 \\u1ecb ra lu\\u00f4n m\\u00e0u c\\u0169 v\\u00e0 b\\u1ea9n nh\\u01b0 \\u0111\\u00e3 m\\u1eb7c_c\\u1ea3 ng\\u00e0n l\\u1ea7n r\\u1ed3i m\\u1edbi b\\u00e1n m\\u00e0u tr\\u00ean \\u1ea3nh v\\u1eabn \\u0111\\u1eb9p nh\\u01b0ng m\\u00e0u \\u1edf ngo\\u00e0i h\\u01a1i v\\u00e0ng nh\\u01b0ng sao size l\\u1edbn kh\\u00f4ng r\\u1ed9ng ch\\u00fat n\\u00e0o m\\u00e0 v\\u1eabn ch\\u1eadt \\u1edf \\u0111\\u00f9i m\\u00ecnh \\u0111\\u00f9i m\\u00ecnh 52\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in - out\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"out\",\n          \"in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"star\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}],"source":["df = pd.concat([df, df_extra])\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":840},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714956146670,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"kESVPUtoEn1h","outputId":"7f42b898-bd05-4bc8-a44a-b85326d500ce"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-2ba127178d25>:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['star'] = df['star'].apply(lambda x: x[0] if isinstance(x, str) else x)\n","<ipython-input-11-2ba127178d25>:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['star'] = df['star'].astype(int)\n","<ipython-input-11-2ba127178d25>:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['clean_cmt'] = df['clean_cmt'].astype(str)\n","<ipython-input-11-2ba127178d25>:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['comment'] = df['clean_cmt'].apply(lambda x: x.replace(\"_\", \" \"))\n"]},{"output_type":"execute_result","data":{"text/plain":["       rating_star        cmtid       itemid  \\\n","0                5  12486152255  23676957049   \n","1                5  12999867370  23754591984   \n","2                4  12360238836  23676957049   \n","3                5  12877630757  23182460672   \n","4                1  12154882262  16694691238   \n","...            ...          ...          ...   \n","14297            1  12198892833  22313943551   \n","14298            2  13257460773  19093790478   \n","14299            1  13061370615   7141012796   \n","14300            5  13194167688  21695044273   \n","14301            4  10421157301   6888422353   \n","\n","                                                    name  \\\n","0      áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n","1      Quần jean ống suông rách gối , Quần bò dáng su...   \n","2      áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...   \n","3      Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...   \n","4      ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...   \n","...                                                  ...   \n","14297   💛 Loại Đẹp 💛 Tất thể thao cổ cao ngắn nam nữ ...   \n","14298   Keith House siêu dễ thương dáng rộng phong cá...   \n","14299         Áo thun cổ tròn lệch vai nhiều màu Ulzzang   \n","14300   Áo vest len cổ chữ V dáng rộng dành cho nữ Áo...   \n","14301   Quần Cullotes vải nữ Quần ống rộng lưng cao Q...   \n","\n","                                                 comment  \\\n","0      xuất sắc luôn giao hàng nhanh giá rẻ đóng gói ...   \n","1      xinh xỉu má ơi cái màu đỉnh cựccc ai do dự thì...   \n","2      xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...   \n","3      xinh lắm luôn á mọi người ơi chất vải mềm lêm ...   \n","4                                  xấu tốt nhất đừng mua   \n","...                                                  ...   \n","14297                            24 nghìn một đôi nhé mẹ   \n","14298                           150 nghìn áo chất quá tệ   \n","14299  <emoji> smiling face with tear </emoji> hình m...   \n","14300  <emoji> smiling face with sunglasses </emoji> ...   \n","14301  <emoji> fire </emoji> <emoji> fire </emoji> <e...   \n","\n","                                               clean_cmt in - out  star  \n","0      xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...       in     5  \n","1      xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...       in     5  \n","2      xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...       in     5  \n","3      xinh lắm luôn á mọi người ơi chất vải mềm lêm ...       in     5  \n","4                                  xấu tốt nhất đừng mua       in     1  \n","...                                                  ...      ...   ...  \n","14297                            24 nghìn một đôi nhé mẹ      out     0  \n","14298                           150 nghìn áo chất quá tệ       in     1  \n","14299  <emoji> smiling face with tear </emoji> hình m...       in     3  \n","14300  <emoji> smiling face with sunglasses </emoji> ...       in     5  \n","14301  <emoji> fire </emoji> <emoji> fire </emoji> <e...      out     0  \n","\n","[28595 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-f452b460-c483-454d-a554-f51a2bcc6f72\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rating_star</th>\n","      <th>cmtid</th>\n","      <th>itemid</th>\n","      <th>name</th>\n","      <th>comment</th>\n","      <th>clean_cmt</th>\n","      <th>in - out</th>\n","      <th>star</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5</td>\n","      <td>12486152255</td>\n","      <td>23676957049</td>\n","      <td>áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...</td>\n","      <td>xuất sắc luôn giao hàng nhanh giá rẻ đóng gói ...</td>\n","      <td>xuất_sắc luôn giao hàng nhanh giá rẻ đóng_gói ...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5</td>\n","      <td>12999867370</td>\n","      <td>23754591984</td>\n","      <td>Quần jean ống suông rách gối , Quần bò dáng su...</td>\n","      <td>xinh xỉu má ơi cái màu đỉnh cựccc ai do dự thì...</td>\n","      <td>xinh xỉu má ơi cái màu đỉnh cựccc ai do_dự thì...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>12360238836</td>\n","      <td>23676957049</td>\n","      <td>áo trễ vai phối bèo tay ngắn,áo sơ mi kiểu nữ ...</td>\n","      <td>xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...</td>\n","      <td>xinh nm hơi rộng huhu phải kẹp đằng sau nựa rấ...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5</td>\n","      <td>12877630757</td>\n","      <td>23182460672</td>\n","      <td>Áo thun tay dài nữ dây rút 2 bên cổ tròn form ...</td>\n","      <td>xinh lắm luôn á mọi người ơi chất vải mềm lêm ...</td>\n","      <td>xinh lắm luôn á mọi người ơi chất vải mềm lêm ...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>12154882262</td>\n","      <td>16694691238</td>\n","      <td>ÁO THUN/T-SHIRT UNISEX FORM ÂU RAGE OF THE SEA...</td>\n","      <td>xấu tốt nhất đừng mua</td>\n","      <td>xấu tốt nhất đừng mua</td>\n","      <td>in</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14297</th>\n","      <td>1</td>\n","      <td>12198892833</td>\n","      <td>22313943551</td>\n","      <td>💛 Loại Đẹp 💛 Tất thể thao cổ cao ngắn nam nữ ...</td>\n","      <td>24 nghìn một đôi nhé mẹ</td>\n","      <td>24 nghìn một đôi nhé mẹ</td>\n","      <td>out</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14298</th>\n","      <td>2</td>\n","      <td>13257460773</td>\n","      <td>19093790478</td>\n","      <td>Keith House siêu dễ thương dáng rộng phong cá...</td>\n","      <td>150 nghìn áo chất quá tệ</td>\n","      <td>150 nghìn áo chất quá tệ</td>\n","      <td>in</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14299</th>\n","      <td>1</td>\n","      <td>13061370615</td>\n","      <td>7141012796</td>\n","      <td>Áo thun cổ tròn lệch vai nhiều màu Ulzzang</td>\n","      <td>&lt;emoji&gt; smiling face with tear &lt;/emoji&gt; hình m...</td>\n","      <td>&lt;emoji&gt; smiling face with tear &lt;/emoji&gt; hình m...</td>\n","      <td>in</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>14300</th>\n","      <td>5</td>\n","      <td>13194167688</td>\n","      <td>21695044273</td>\n","      <td>Áo vest len cổ chữ V dáng rộng dành cho nữ Áo...</td>\n","      <td>&lt;emoji&gt; smiling face with sunglasses &lt;/emoji&gt; ...</td>\n","      <td>&lt;emoji&gt; smiling face with sunglasses &lt;/emoji&gt; ...</td>\n","      <td>in</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>14301</th>\n","      <td>4</td>\n","      <td>10421157301</td>\n","      <td>6888422353</td>\n","      <td>Quần Cullotes vải nữ Quần ống rộng lưng cao Q...</td>\n","      <td>&lt;emoji&gt; fire &lt;/emoji&gt; &lt;emoji&gt; fire &lt;/emoji&gt; &lt;e...</td>\n","      <td>&lt;emoji&gt; fire &lt;/emoji&gt; &lt;emoji&gt; fire &lt;/emoji&gt; &lt;e...</td>\n","      <td>out</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>28595 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f452b460-c483-454d-a554-f51a2bcc6f72')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f452b460-c483-454d-a554-f51a2bcc6f72 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f452b460-c483-454d-a554-f51a2bcc6f72');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7e547c7f-d312-4a77-9c95-c2d771a217b5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e547c7f-d312-4a77-9c95-c2d771a217b5')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7e547c7f-d312-4a77-9c95-c2d771a217b5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 28595,\n  \"fields\": [\n    {\n      \"column\": \"rating_star\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cmtid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1425412433,\n        \"min\": 1015612654,\n        \"max\": 13347066052,\n        \"num_unique_values\": 14298,\n        \"samples\": [\n          12472224975,\n          12687376683,\n          10814177612\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"itemid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5196475903,\n        \"min\": 1592182206,\n        \"max\": 25450823380,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          19692270976,\n          23970387156,\n          25450823380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 292,\n        \"samples\": [\n          \"\\u00c1o N\\u1ec9 Sweater ch\\u1eef IUS Ph\\u1ed1i Th\\u00eau Da H\\u1ed3ng , \\u00c1o N\\u1ec9 C\\u1ed5 Tr\\u00f2n M\\u00e0u X\\u00e1m Ti\\u00eau From R\\u1ed9ng Tay B\\u1ed3ng Unsiex M67\",\n          \"M\\u1eabu m\\u1edbi si\\u00eau hot \\u00c1o hoodie in ENG, \\u00e1o n\\u1ec9 d\\u00e1ng r\\u1ed9ng c\\u00f3 m\\u0169 tr\\u00f9m \\u0111\\u1ea7u 2 l\\u1edbp, d\\u00e2y r\\u00fat d\\u00e0y d\\u1eb7n\",\n          \"A\\u0301o N\\u1eef, \\u00c1o Hoodie \\u0110\\u00ednh V\\u1ea3i B\\u00f2 In Tho\\u0309 Ch\\u00e2\\u0301t Ni\\u0309 Ta\\u0300u Si\\u00eau Cute Hottrend 2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28278,\n        \"samples\": [\n          \"kh\\u00f4ng gi\\u1ed1ng h\\u00ecnh \\u1ed1ng r\\u1ea5t b\\u1ef1 hai ng\\u01b0\\u1eddi chui v\\u00f4 c\\u0169ng \\u0111\\u01b0\\u1ee3c n\\u1eefa size m\",\n          \"\\u0111\\u00fang v\\u1edbi m\\u00f4 t\\u1ea3 \\u0111\\u00fang ch\\u1ea5t li\\u1ec7u kh\\u00f4ng bi\\u1ebft m\\u00e0u s\\u1eafc \\u0111en \\u0111\\u1eb9p \\u1ea5m d\\u00e0y giao h\\u00e0ng nhanh s\\u1ea3n ph\\u1ea9m r\\u1ea5t ok l\\u1ea7n sau s\\u1ebd \\u1ee7ng h\\u1ed9 shop ti\\u1ebfp\",\n          \"\\u0111\\u00fang m\\u00f4 t\\u1ea3 true m\\u00e0u s\\u1eafc be h\\u01a1i v\\u00e0ng \\u00e1o \\u0111\\u1eb9p v\\u1edbi gi\\u00e1 n\\u00e0y th\\u00ec v\\u1ea3i c\\u0169ng ok m\\u0169 hai l\\u1edbp c\\u00f3 n\\u00ean muaaa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_cmt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28278,\n        \"samples\": [\n          \"kh\\u00f4ng gi\\u1ed1ng h\\u00ecnh \\u1ed1ng r\\u1ea5t b\\u1ef1 hai ng\\u01b0\\u1eddi chui v\\u00f4 c\\u0169ng \\u0111\\u01b0\\u1ee3c n\\u1eefa size m\",\n          \"\\u0111\\u00fang v\\u1edbi m\\u00f4_t\\u1ea3 \\u0111\\u00fang ch\\u1ea5t_li\\u1ec7u kh\\u00f4ng bi\\u1ebft m\\u00e0u s\\u1eafc \\u0111en \\u0111\\u1eb9p \\u1ea5m d\\u00e0y giao h\\u00e0ng nhanh s\\u1ea3n_ph\\u1ea9m r\\u1ea5t ok l\\u1ea7n sau s\\u1ebd \\u1ee7ng_h\\u1ed9 shop ti\\u1ebfp\",\n          \"\\u0111\\u00fang m\\u00f4_t\\u1ea3 true m\\u00e0u_s\\u1eafc be h\\u01a1i v\\u00e0ng \\u00e1o \\u0111\\u1eb9p v\\u1edbi gi\\u00e1 n\\u00e0y th\\u00ec v\\u1ea3i c\\u0169ng ok m\\u0169 hai l\\u1edbp c\\u00f3 n\\u00ean muaaa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"in - out\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"out\",\n          \"in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"star\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          5,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}],"source":["df = df.dropna(subset=['star'])\n","df['star'] = df['star'].apply(lambda x: x[0] if isinstance(x, str) else x)\n","df['star'] = df['star'].astype(int)\n","df['clean_cmt'] = df['clean_cmt'].astype(str)\n","df['comment'] = df['clean_cmt'].apply(lambda x: x.replace(\"_\", \" \"))\n","df"]},{"cell_type":"markdown","metadata":{"id":"F5La1eLNRl7e"},"source":["# Classify domain"]},{"cell_type":"markdown","metadata":{"id":"k_63NDZJSwoF"},"source":["### Deal with imbalanced data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kf6NV3ugcLvQ"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_dGiMDkYBsO"},"outputs":[],"source":["from imblearn.under_sampling import RandomUnderSampler\n","\n","def get_df_train(df, label, imbalance = False):\n","  if imbalance:\n","    rus = RandomUnderSampler(random_state=0)\n","    df[label] = df[label].astype(str)\n","    X_resampled, y_resampled = rus.fit_resample(np.reshape(df.index, (-1, 1)), df[label])\n","    df_domain_balance = df.iloc[X_resampled.ravel()]\n","  else:\n","    df_domain_balance = df\n","\n","\n","  df_train = df_domain_balance[['comment', label, 'clean_cmt']]\n","  df_train.rename(columns={\n","                                'comment': 'content',\n","                                label: 'classname'}, inplace=True)\n","\n","  df_train['clean_cmt'] = df_train['clean_cmt'].astype(str)\n","  df_train['content'] = df_train['content'].astype(str)\n","\n","  df_train = df_train.reset_index(drop = True)\n","  return df_train\n"]},{"cell_type":"markdown","metadata":{"id":"SqclUtPbg2nH"},"source":["# Define general function to train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxeCckrPg4hX"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score, recall_score\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"markdown","metadata":{"id":"_DQWQn0ipo57"},"source":["#### Convert text to vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10129,"status":"ok","timestamp":1714956185193,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"RRCNq5kEibb2","outputId":"e37168de-fd38-469d-9118-98d47d82f601"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import numpy as np\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from transformers import AutoTokenizer, AutoModel\n","from more_itertools import chunked\n","import torch\n","\n","class TransformerEmbedding(TransformerMixin, BaseEstimator):\n","\n","    def __init__(self, model_name='bert-base-uncased', batch_size=1, layer=-1):\n","        self.model_name = model_name\n","        self.layer = layer\n","        self.batch_size = batch_size\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","        self.model = AutoModel.from_pretrained(self.model_name).to(self.device)\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        res = []\n","        total = 0\n","        for batch in chunked(X, self.batch_size):\n","            encoded_input = self.tokenizer(\n","                batch,\n","                max_length = 256,\n","                return_tensors='pt',\n","                padding = 'max_length',\n","                truncation = True,\n","                pad_to_max_length = True,\n","                add_special_tokens = True,\n","            ).to(self.device)\n","\n","            with torch.no_grad():\n","              output = self.model(**encoded_input)\n","              embed = output.last_hidden_state[:,-1].cpu().detach().numpy()\n","              res.append(embed)\n","        return np.concatenate(res)\n","\n","\n","model_name = 'vinai/phobert-base-v2'\n","\n","bert_embed = TransformerEmbedding(model_name, batch_size = 128)"]},{"cell_type":"markdown","metadata":{"id":"Y3xC09gWpx3o"},"source":["#### Tokenize text and encode label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9VFOWXNo1tX"},"outputs":[],"source":["def encode_X(X):\n","  X_encode = bert_embed.transform(X)\n","  return X_encode\n","\n","def encode_class(labels):\n","  label_encoder = LabelEncoder()\n","  y = label_encoder.fit_transform(labels)\n","\n","  return y"]},{"cell_type":"markdown","metadata":{"id":"LW8y-8Mnq2G_"},"source":["#### Data scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8VPJJDjqThO"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","def scale(data):\n","  scaler = MinMaxScaler()\n","  data = scaler.fit_transform(data)\n","\n","  return data"]},{"cell_type":"markdown","metadata":{"id":"d0_SAZAWq6yP"},"source":["#### Machine Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDF_xSTQccNS"},"outputs":[],"source":["import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AN24V3MrKwV"},"outputs":[],"source":["def ml_train(X, y, skf):\n","    model = [MultinomialNB(), LogisticRegression(), LinearSVC()]\n","    table_ml = []\n","\n","    X = scale(X)\n","\n","    for i in model:\n","        start = time.time()\n","        cross_val_scores = cross_val_score(i, X, y, cv=skf, scoring='accuracy')\n","\n","        y_pred = cross_val_predict(i, X, y, cv=skf)\n","\n","        f1 = f1_score(y, y_pred, average='weighted')\n","        recall = recall_score(y, y_pred, average='weighted')\n","\n","        end = time.time()\n","        table_ml.append({\n","            \"Model Name\": type(i).__name__,\n","            \"Mean accuracy\": cross_val_scores.mean(),\n","            \"F1 Score\": f1,\n","            \"Recall\": recall,\n","            \"Training time\": end-start\n","        })\n","\n","    result_df = pd.DataFrame(table_ml)\n","    result_df.set_index(\"Model Name\", inplace=True)\n","\n","    print(result_df)"]},{"cell_type":"markdown","metadata":{"id":"7Vhkr7fX9-e9"},"source":["#### Deep Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3G0pDu1K-C0g"},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGfJ6cdx-2UL"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","def dl_train(X, y, skf, num_class):\n","    accuracies = []\n","    f1_scores = []\n","    recalls = []\n","\n","    start = time.time()\n","\n","    for train_index, test_index in skf.split(X, y):\n","        y_train_one_hot = to_categorical(y, num_classes=num_class)\n","\n","        X_train, X_test = X[train_index], X[test_index]\n","        y_train, y_test = y_train_one_hot[train_index], y_train_one_hot[test_index]\n","\n","        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","        # Build CNN model\n","        model = Sequential(\n","          [Conv1D(128, 2, activation='relu'),\n","          MaxPooling1D(),\n","          Conv1D(128, 3, activation='relu'),\n","          MaxPooling1D(),\n","          Conv1D(128, 4, activation='relu'),\n","          MaxPooling1D(),\n","          Flatten(),\n","          Dense(256, activation='relu'),\n","          Dense(num_class, activation='sigmoid')\n","        ])\n","\n","        model.compile(optimizer='adam', loss='categorical_crossentropy')\n","\n","        # # Huấn luyện mô hình\n","        model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=0)\n","\n","        # Đánh giá mô hình\n","        y_pred = model.predict(X_test)\n","\n","        y_pred = np.argmax(y_pred, axis = 1)\n","        y_test = np.argmax(y_test, axis = 1)\n","\n","        f1 = f1_score(y_test, y_pred, average='weighted')\n","        accuracy = accuracy_score(y_test, y_pred)\n","        recall = recall_score(y_test, y_pred,  average='weighted')\n","\n","        f1_scores.append(f1)\n","        accuracies.append(accuracy)\n","        recalls.append(recall)\n","\n","    avg_accuracy = sum(accuracies)/len(accuracies)\n","    avg_f1 = sum(f1_scores)/len(f1_scores)\n","    avg_recall = sum(recalls)/len(recalls)\n","    end = time.time()\n","\n","    print(\"Average Accuracy: \", avg_accuracy)\n","    print(\"Average F1 Score: \", avg_f1)\n","    print(\"Average Recall: \", avg_recall)\n","    print(\"Training time: \", end-start)"]},{"cell_type":"markdown","metadata":{"id":"RnINxfUkFP9Q"},"source":["#### Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGI3OedRFMs1"},"outputs":[],"source":["import gc\n","\n","def free_gpu():\n","  torch.cuda.empty_cache()\n","  gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fz_yOP_3FYIH"},"outputs":[],"source":["import os, pickle, re, keras, sklearn, string, io, gensim, warnings, io\n","import random as rn\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"HKFlj9jCFfHx"},"source":["##### Normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-Xd1HyaFc-y"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","import torch.nn.functional as F\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","def get_train_test(train_data, word = True):\n","\n","    if word:\n","        X = train_data['clean_cmt'].values.tolist()\n","    else:\n","        X = train_data['content'].values.tolist()\n","\n","    num_class = train_data['classname'].nunique()\n","    labels = train_data['classname'].unique()\n","\n","    y = encode_class(train_data['classname'].tolist())\n","    y_onehot = F.one_hot(torch.tensor(y), num_classes=num_class).numpy()\n","\n","    X_encode = encode_X(X)\n","    y = np.array(y)\n","\n","    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n","\n","    return X, y, X_encode, y_onehot, num_class, labels, class_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16020,"status":"ok","timestamp":1714956204058,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"_2mgvV2vF8hE","outputId":"04cc2ac8-cda6-422e-beea-c85b495b6760"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}],"source":["!pip install sentencepiece\n","!pip install accelerate\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYnkwq5EF-vX"},"outputs":[],"source":["from torch.nn import *\n","from transformers import *\n","import torch\n","import random,operator\n","import pickle, statistics\n","from sklearn.metrics import *\n","import math\n","from torch.utils.data import DataLoader\n","from datasets import load_metric\n","metric = load_metric('accuracy') # Acccuracy\n","from transformers import logging\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1714956212689,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"s616ero4HBIL","outputId":"ee80dd3f-3721-4b4d-9426-b74332852e74"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon May  6 00:43:31 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0              25W /  70W |    677MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["from torch import cuda\n","device = 'cuda'if cuda.is_available() else 'cpu'\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9mxHCToHEWr"},"outputs":[],"source":["seed = 42\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGv-EqCDHIbA"},"outputs":[],"source":["# Convert label to id\n","\n","def convert_lb2id(label):\n","  label2id = {}\n","  for idx, i in enumerate(label):\n","    label2id[i] = idx\n","\n","  return label2id\n","\n","def convert_id2lb(label):\n","  id2label = {}\n","  for idx, i in enumerate(label):\n","    id2label[idx] = i\n","\n","  return id2label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_-4BLr9HP_4"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","# structure dataset for training\n","\n","class CustomDataset(Dataset):\n","    def __init__ (self, tokenizer, X_data, y_data = None):\n","        self.tokenizer = tokenizer\n","        self.X_data = X_data\n","        self.y_data = y_data\n","\n","    def __len__ (self):\n","        return len(self.X_data)\n","\n","    def __getitem__(self, idx):\n","        inputs = self.tokenizer.encode_plus(\n","            self.X_data[idx],\n","            max_length=256,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        return_data = {\n","            'input_ids' : inputs.input_ids.squeeze(),\n","            'attention_mask': inputs.attention_mask.squeeze(),\n","        }\n","\n","        if self.y_data is not None:\n","          return_data['labels'] = torch.FloatTensor(self.y_data[idx])\n","\n","        return return_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQUMlWZ1HQ-p"},"outputs":[],"source":["# create object to train\n","\n","class CustomTrainer(Trainer):\n","    def __init__(self, class_weights, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n","\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","\n","        if \"labels\" in inputs:\n","          labels = inputs.pop(\"labels\")\n","\n","        # Forward\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","\n","        #  Compute loss\n","        loss_function = CrossEntropyLoss(weight = self.class_weights)       # MCC\n","        # loss_function = BCEWithLogitsLoss()    # MLC\n","\n","        if self.args.past_index >= 0:\n","            self._past = outputs[self.args.past_index]\n","\n","        if labels is not None:\n","            loss = loss_function(logits, labels)\n","        else:\n","            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n","            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n","\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeVlCcFdHU9q"},"outputs":[],"source":["# Get dataset to train\n","\n","def get_dataset(tokenizer, X_train, X_test, y_train, y_test):\n","  train_dataset = CustomDataset(tokenizer, X_train, y_train)\n","  if type_job == \"train\":\n","    test_dataset = CustomDataset(tokenizer, X_test, y_test)\n","  else:\n","    test_dataset = CustomDataset(tokenizer, X_test)\n","\n","\n","  test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","  del test_dataset\n","\n","  return train_dataset, test_dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iFeHdZIHXU6"},"outputs":[],"source":["# Get model to train\n","\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, AutoModelForSequenceClassification, AutoModel\n","def get_model(name_model, labels):\n","  tokenizer = AutoTokenizer.from_pretrained(name_model)\n","  model = AutoModelForSequenceClassification.from_pretrained(\n","            name_model,\n","            num_labels = len(labels),\n","            label2id = convert_lb2id(labels),\n","            id2label = convert_id2lb(labels),\n","            problem_type = problem_type,\n","            ignore_mismatched_sizes=True\n","          ).to(device)\n","\n","  return model, tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BreTOLZXHb2F"},"outputs":[],"source":["# Apply model\n","\n","def train_model(model, class_weights, training_args, train_dataset):\n","  model.train()\n","  trainer = CustomTrainer(\n","      class_weights=class_weights,\n","      model = model,\n","      args = training_args,\n","      train_dataset = train_dataset,\n","      # eval_dataset = test_dataset,\n","      data_collator = lambda data : {\n","          'input_ids' : torch.stack([item['input_ids'] for item in data]),\n","          'attention_mask' : torch.stack([item['attention_mask'] for item in data]),\n","          'labels' : torch.stack([item['labels'] for item in data]),\n","      }\n","  )\n","  trainer.train()\n","\n","  del trainer\n","\n","  free_gpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbDjUoPVIDCR"},"outputs":[],"source":["# Convert to one-hot-vector (index to num)\n","\n","def get_ohv(label, labels):\n","  ohv_label = [0] * len(labels)\n","  ohv_label[label] = 1\n","\n","  return ohv_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NC6lmQyIL1F"},"outputs":[],"source":["# Get result\n","\n","def print_predict_result(model, test_dataloader, labels):\n","  y_pred = []\n","  y_true = []\n","\n","  model.eval()\n","  with torch.no_grad():\n","      for batch in test_dataloader:\n","          input_ids = batch['input_ids'].to(device)\n","          attention_mask = batch['attention_mask'].to(device)\n","\n","          output = model(input_ids, attention_mask=attention_mask)\n","\n","          logits = output.logits.detach().cpu().numpy()\n","          logits = np.array([np.argmax(i).flatten().item() for i in logits])\n","\n","          y_pred.extend(logits)\n","\n","          if type_job == \"train\":\n","            true_labels = batch['labels'].cpu().numpy()\n","            true_labels = [np.argmax(i) for i in true_labels]\n","            y_true.extend(true_labels)\n","\n","  y_pred = [get_ohv(i, labels) for i in y_pred]\n","  y_true = [get_ohv(i, labels) for i in y_true]\n","\n","  micro_test = f1_score(y_true, y_pred, average='micro')\n","  macro_test = f1_score(y_true, y_pred, average='macro')\n","  weighted_test = f1_score(y_true, y_pred, average='weighted')\n","  accuracy_test = accuracy_score(y_true,y_pred)\n","  report = classification_report(y_true, y_pred, target_names = labels)\n","  print()\n","  print(\"Accuracy: \", round(accuracy_test*100,2))\n","  print(\"F1 weighted: \", round(weighted_test*100,2))\n","  print(\"f1 macro: \", round(macro_test*100,2))\n","  print(\"f1 micro: \", round(micro_test*100,2))\n","\n","  print(\"-\"*100)\n","  print(report)\n","  print()\n","  print()\n","  print()\n","  print(\"=\" * 100 + \"\\n\")\n","\n","  return accuracy_test, weighted_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lsFaI4CIROY"},"outputs":[],"source":["#Prepare\n","\n","name_model = \"vinai/phobert-base-v2\"\n","problem_type = \"single_label_classification\"\n","\n","max_len = 256 #Độ dài tối đa mô hình có thể nhận vào\n","epochs = 5 #Số lần huấn luyện\n","batch_size = 64\n","learning_rate = 1e-4 #Tốc độ học\n","weight_decay = 0.01 #Phần trăm bỏ để tránh overfitting\n","\n","log_strategy = \"steps\"\n","log_step = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1714956212691,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"_3xX6wnEIY6C","outputId":"7dd50581-c844-4748-dd6e-656e31061f22"},"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["training_args = TrainingArguments(\n","    output_dir = '/content',\n","    # evaluation_strategy= 'epoch',\n","    learning_rate = learning_rate,\n","    # save_strategy = 'epoch',\n","    per_device_train_batch_size = batch_size,\n","    num_train_epochs = epochs,\n","    weight_decay = weight_decay,\n","    fp16=True,\n","    fp16_full_eval=True,\n","    jit_mode_eval=True,\n","    logging_strategy=log_strategy,\n","    logging_steps = log_step,\n","    seed = seed,\n","    # load_best_model_at_end=True,\n",")\n","\n","type_job = \"train\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXxPdVJ7xGpi"},"outputs":[],"source":["def transformer_train(name_model, labels, X, y, skf, class_weights):\n","  X = np.array(X)\n","\n","  eval_data = {}\n","  for idx, (train_index, test_index) in enumerate(skf.split(X, np.argmax(y, axis = 1))):\n","      X_train_fold, X_test_fold = X[train_index], X[test_index]\n","      y_train_fold, y_test_fold = y[train_index], y[test_index]\n","\n","      free_gpu()\n","      model, tokenizer = get_model(name_model, labels)\n","      train_dataset, test_dataloader = get_dataset(tokenizer, X_train_fold, X_test_fold, y_train_fold, y_test_fold)\n","\n","      start = time.time()\n","      train_model(model, class_weights, training_args, train_dataset)\n","      end = time.time()\n","      accuracy_test, weighted_test = print_predict_result(model, test_dataloader, labels)\n","      print(\"Training time: \", end-start)\n","\n","      print()\n","\n","      eval_data[idx] = [accuracy_test, weighted_test]\n","\n","      del model\n","\n","  print(eval_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nk0t02POu3xH"},"outputs":[],"source":["num_folds = 5\n","skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"HsGsUeFILNrR"},"source":["# Train in-out domain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkvpIcTouMEG"},"outputs":[],"source":["df_inout = get_df_train(df, label = 'in - out', imbalance = True)"]},{"cell_type":"markdown","metadata":{"id":"FesbhUCmLcNy"},"source":["## Token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TobMc7J3La-0"},"outputs":[],"source":["X, y, X_encode, y_onehot, num_class, labels, class_weights = get_train_test(df_inout, word = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54156,"status":"ok","timestamp":1714908110501,"user":{"displayName":"Uyên Nguyễn Ngọc Phương","userId":"06000761974539573354"},"user_tz":-420},"id":"d7Axi7eUv78u","outputId":"f9afbef4-1ec3-4738-d9da-dc1a1a20d557"},"outputs":[{"name":"stdout","output_type":"stream","text":["                    Mean accuracy  F1 Score    Recall  Training time\n","Model Name                                                          \n","MultinomialNB            0.775645  0.784400  0.775658       0.468863\n","LogisticRegression       0.917934  0.916757  0.917933       9.687179\n","LinearSVC                0.930532  0.931689  0.932012      43.535036\n"]}],"source":["ml_train(X_encode, y, skf)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59510,"status":"ok","timestamp":1714908343067,"user":{"displayName":"Uyên Nguyễn Ngọc Phương","userId":"06000761974539573354"},"user_tz":-420},"id":"M1H9TxYKvNvt","outputId":"34f91d8a-6b04-4053-ee26-7d28eb7080f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["34/34 [==============================] - 1s 7ms/step\n","34/34 [==============================] - 0s 4ms/step\n","34/34 [==============================] - 0s 4ms/step\n","34/34 [==============================] - 0s 8ms/step\n","34/34 [==============================] - 0s 4ms/step\n","Average Accuracy:  0.9275689081110767\n","Average F1 Score:  0.927324989037918\n","Average Recall:  0.9275689081110767\n","Training time:  60.054502964019775\n"]}],"source":["dl_train(X_encode, y, skf, num_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1735464,"status":"ok","timestamp":1714912705791,"user":{"displayName":"Uyên Nguyễn Ngọc Phương","userId":"06000761974539573354"},"user_tz":-420},"id":"R8Am1cVjybwR","outputId":"dabd6334-9686-4620-fae2-c2f6410d7fed"},"outputs":[{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,318\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:44, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.428100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.344400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.348800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.267700</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.325900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.166500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  92.59\n","F1 weighted:  92.66\n","f1 macro:  88.18\n","f1 micro:  92.59\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.96      0.95      0.95       875\n","         out       0.79      0.83      0.81       205\n","\n","   micro avg       0.93      0.93      0.93      1080\n","   macro avg       0.88      0.89      0.88      1080\n","weighted avg       0.93      0.93      0.93      1080\n"," samples avg       0.93      0.93      0.93      1080\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  346.2096788883209\n","\n"]},{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,318\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:37, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.483100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.285700</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.221500</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.192000</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.104600</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.068200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  95.65\n","F1 weighted:  95.64\n","f1 macro:  92.91\n","f1 micro:  95.65\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.97      0.97      0.97       875\n","         out       0.89      0.88      0.89       205\n","\n","   micro avg       0.96      0.96      0.96      1080\n","   macro avg       0.93      0.93      0.93      1080\n","weighted avg       0.96      0.96      0.96      1080\n"," samples avg       0.96      0.96      0.96      1080\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  339.6013090610504\n","\n"]},{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,318\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:36, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.407400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.262400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.218600</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.128500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.072300</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.053700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  96.3\n","F1 weighted:  96.25\n","f1 macro:  93.82\n","f1 micro:  96.3\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.97      0.99      0.98       875\n","         out       0.93      0.87      0.90       205\n","\n","   micro avg       0.96      0.96      0.96      1080\n","   macro avg       0.95      0.93      0.94      1080\n","weighted avg       0.96      0.96      0.96      1080\n"," samples avg       0.96      0.96      0.96      1080\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  338.26355719566345\n","\n"]},{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,319\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:31, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.451600</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.286500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.358400</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.290800</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.218400</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.193100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  94.44\n","F1 weighted:  94.45\n","f1 macro:  91.0\n","f1 micro:  94.44\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.97      0.96      0.97       874\n","         out       0.85      0.86      0.85       205\n","\n","   micro avg       0.94      0.94      0.94      1079\n","   macro avg       0.91      0.91      0.91      1079\n","weighted avg       0.94      0.94      0.94      1079\n"," samples avg       0.94      0.94      0.94      1079\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  333.7113227844238\n","\n"]},{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,319\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:31, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.426900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.345400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.317200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.261500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.209700</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.175300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  95.37\n","F1 weighted:  95.37\n","f1 macro:  92.47\n","f1 micro:  95.37\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.97      0.97      0.97       874\n","         out       0.88      0.88      0.88       205\n","\n","   micro avg       0.95      0.95      0.95      1079\n","   macro avg       0.92      0.92      0.92      1079\n","weighted avg       0.95      0.95      0.95      1079\n"," samples avg       0.95      0.95      0.95      1079\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  333.04169821739197\n","\n","{0: [0.9259259259259259, 0.926594903606398], 1: [0.9564814814814815, 0.9564407071200256], 2: [0.9629629629629629, 0.962455240233018], 3: [0.9443929564411492, 0.9444960767440154], 4: [0.953660797034291, 0.953660797034291]}\n"]}],"source":["transformer_train(name_model, labels, X, y_onehot, skf, class_weights)"]},{"cell_type":"markdown","metadata":{"id":"j9igcA7gLevQ"},"source":["## No token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iC31Y57BLgjO"},"outputs":[],"source":["X, y, X_encode, y_onehot, num_class, labels, class_weights = get_train_test(df_inout, word = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45115,"status":"ok","timestamp":1714912872733,"user":{"displayName":"Uyên Nguyễn Ngọc Phương","userId":"06000761974539573354"},"user_tz":-420},"id":"BbecJ0H1wW1v","outputId":"82436c68-b9d4-44a0-f49e-7e2157ab7e46"},"outputs":[{"name":"stdout","output_type":"stream","text":["                    Mean accuracy  F1 Score    Recall  Training time\n","Model Name                                                          \n","MultinomialNB            0.782502  0.793012  0.782512       0.231308\n","LogisticRegression       0.914971  0.913178  0.914969       5.862571\n","LinearSVC                0.927936  0.929061  0.929048      38.980738\n"]}],"source":["ml_train(X_encode, y, skf)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52320,"status":"ok","timestamp":1714913216879,"user":{"displayName":"Uyên Nguyễn Ngọc Phương","userId":"06000761974539573354"},"user_tz":-420},"id":"oCWUzSxLwWUr","outputId":"cb33d541-48ac-4a8e-ccea-d7c0f0b83055"},"outputs":[{"name":"stdout","output_type":"stream","text":["34/34 [==============================] - 0s 4ms/step\n","34/34 [==============================] - 0s 3ms/step\n","34/34 [==============================] - 0s 3ms/step\n","34/34 [==============================] - 0s 3ms/step\n","34/34 [==============================] - 0s 4ms/step\n","Average Accuracy:  0.9257172278859025\n","Average F1 Score:  0.9263021222200255\n","Average Recall:  0.9257172278859025\n","Training time:  51.96348857879639\n"]}],"source":["dl_train(X_encode, y, skf, num_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1695708,"status":"ok","timestamp":1714916476542,"user":{"displayName":"Thư Phạm Anh","userId":"01186629817714377316"},"user_tz":-420},"id":"21mR28Zs_4bl","outputId":"4d102fe0-73d4-4bc0-a67a-ef7aa525db55"},"outputs":[{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,318\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:24, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.502200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.329400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.252300</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.174300</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.138800</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.092900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  94.72\n","F1 weighted:  94.68\n","f1 macro:  91.27\n","f1 micro:  94.72\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.96      0.97      0.97       875\n","         out       0.88      0.84      0.86       205\n","\n","   micro avg       0.95      0.95      0.95      1080\n","   macro avg       0.92      0.91      0.91      1080\n","weighted avg       0.95      0.95      0.95      1080\n"," samples avg       0.95      0.95      0.95      1080\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  326.83907985687256\n","\n"]},{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,318\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:26, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.483100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.285700</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.221500</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.192000</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.104600</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.068200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  95.65\n","F1 weighted:  95.64\n","f1 macro:  92.91\n","f1 micro:  95.65\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.97      0.97      0.97       875\n","         out       0.89      0.88      0.89       205\n","\n","   micro avg       0.96      0.96      0.96      1080\n","   macro avg       0.93      0.93      0.93      1080\n","weighted avg       0.96      0.96      0.96      1080\n"," samples avg       0.96      0.96      0.96      1080\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  327.5147726535797\n","\n"]},{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,318\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:32, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.407400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.262400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.218600</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.128500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.072300</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.053700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  96.3\n","F1 weighted:  96.25\n","f1 macro:  93.82\n","f1 micro:  96.3\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.97      0.99      0.98       875\n","         out       0.93      0.87      0.90       205\n","\n","   micro avg       0.96      0.96      0.96      1080\n","   macro avg       0.95      0.93      0.94      1080\n","weighted avg       0.96      0.96      0.96      1080\n"," samples avg       0.96      0.96      0.96      1080\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  334.0230700969696\n","\n"]},{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,319\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:31, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.451600</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.286500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.358400</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.290800</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.218400</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.193100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  94.44\n","F1 weighted:  94.45\n","f1 macro:  91.0\n","f1 micro:  94.44\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.97      0.96      0.97       874\n","         out       0.85      0.86      0.85       205\n","\n","   micro avg       0.94      0.94      0.94      1079\n","   macro avg       0.91      0.91      0.91      1079\n","weighted avg       0.94      0.94      0.94      1079\n"," samples avg       0.94      0.94      0.94      1079\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  332.7592203617096\n","\n"]},{"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"in\",\n","    \"1\": \"out\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"in\": 0,\n","    \"out\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 4,319\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 675\n","  Number of trainable parameters = 134,999,810\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [675/675 05:31, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.426900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.345400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.317200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.261500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.209700</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.175300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy:  95.37\n","F1 weighted:  95.37\n","f1 macro:  92.47\n","f1 micro:  95.37\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","          in       0.97      0.97      0.97       874\n","         out       0.88      0.88      0.88       205\n","\n","   micro avg       0.95      0.95      0.95      1079\n","   macro avg       0.92      0.92      0.92      1079\n","weighted avg       0.95      0.95      0.95      1079\n"," samples avg       0.95      0.95      0.95      1079\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  333.2050426006317\n","\n","{0: [0.9472222222222222, 0.946764972793208], 1: [0.9564814814814815, 0.9564407071200256], 2: [0.9629629629629629, 0.962455240233018], 3: [0.9443929564411492, 0.9444960767440154], 4: [0.953660797034291, 0.953660797034291]}\n"]}],"source":["transformer_train(name_model, labels, X, y_onehot, skf, class_weights)"]},{"cell_type":"markdown","metadata":{"id":"UjhTgWKNMFTu"},"source":["# Train rating (5 stars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQWR_j-QuRzl"},"outputs":[],"source":["df_star = get_df_train(df[df['star'] != 0], label = 'star')"]},{"cell_type":"markdown","metadata":{"id":"GNzwNA1TMJRw"},"source":["## Token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7Y1Z0PIMIvV"},"outputs":[],"source":["X, y, X_encode, y_onehot, num_class, labels, class_weights = get_train_test(df_star, word = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2162047,"status":"ok","timestamp":1714913664011,"user":{"displayName":"Tâm Trần Anh","userId":"14636975420107569711"},"user_tz":-420},"id":"8ayvp98rK-JG","outputId":"104301e8-216a-46f3-c451-9502e6c24543"},"outputs":[{"name":"stdout","output_type":"stream","text":["                    Mean accuracy  F1 Score    Recall  Training time\n","Model Name                                                          \n","MultinomialNB            0.331064  0.248430  0.331063       1.590490\n","LogisticRegression       0.621885  0.610471  0.621885      82.776465\n","LinearSVC                0.620995  0.608853  0.623123    2077.414481\n"]}],"source":["ml_train(X_encode, y, skf)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210812,"status":"ok","timestamp":1714914302074,"user":{"displayName":"Tâm Trần Anh","userId":"14636975420107569711"},"user_tz":-420},"id":"7trkbpV2LCdq","outputId":"b265ee37-51ce-49cd-ad8e-5a1e5cbbc581"},"outputs":[{"name":"stdout","output_type":"stream","text":["162/162 [==============================] - 1s 4ms/step\n","162/162 [==============================] - 1s 3ms/step\n","162/162 [==============================] - 1s 3ms/step\n","162/162 [==============================] - 1s 3ms/step\n","162/162 [==============================] - 1s 4ms/step\n","Average Accuracy:  0.6248256529908295\n","Average F1 Score:  0.619327579824927\n","Average Recall:  0.6248256529908295\n","Training time:  209.83776569366455\n"]}],"source":["dl_train(X_encode, y, skf, num_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jo3zYZkRPVm7"},"outputs":[],"source":["labels = [str(i) for i in labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7194389,"status":"ok","timestamp":1714935758824,"user":{"displayName":"Ánh Nguyễn Hồng Ngọc","userId":"02917448938706811791"},"user_tz":-420},"id":"MBqVDnEcymyx","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"bbc7e79a-d491-41bd-aa23-bcb94d13171c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,675\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 23:33, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.133000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.946500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.882500</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.787500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.794000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.762100</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.682500</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.623300</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.640400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.565900</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.469300</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.470500</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.460300</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.351200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.336400</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.328800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  74.56\n","F1 weighted:  74.91\n","f1 macro:  71.69\n","f1 micro:  74.56\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.81      0.76      0.79       665\n","           1       0.58      0.65      0.61       664\n","           2       0.68      0.61      0.64       988\n","           3       0.60      0.71      0.65       971\n","           4       0.92      0.86      0.89      1881\n","\n","   micro avg       0.75      0.75      0.75      5169\n","   macro avg       0.72      0.72      0.72      5169\n","weighted avg       0.76      0.75      0.75      5169\n"," samples avg       0.75      0.75      0.75      5169\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1415.3407330513\n","\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,675\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 23:28, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.089200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.895400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.838500</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.743700</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.733600</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.718300</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.641600</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.565200</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.559400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.493600</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.399400</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.386600</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.354800</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.276700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.235900</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.255600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  75.31\n","F1 weighted:  75.42\n","f1 macro:  71.74\n","f1 micro:  75.31\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.77      0.78      0.77       665\n","           1       0.59      0.59      0.59       664\n","           2       0.69      0.63      0.66       988\n","           3       0.63      0.71      0.67       971\n","           4       0.92      0.89      0.90      1881\n","\n","   micro avg       0.75      0.75      0.75      5169\n","   macro avg       0.72      0.72      0.72      5169\n","weighted avg       0.76      0.75      0.75      5169\n"," samples avg       0.75      0.75      0.75      5169\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1410.4508814811707\n","\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,675\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 23:18, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.101200</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.918400</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.862000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.795100</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.762000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.741000</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.665200</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.566100</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.596300</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.536400</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.428300</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.433500</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.412000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.310700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.277500</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.286200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  75.62\n","F1 weighted:  75.89\n","f1 macro:  72.87\n","f1 micro:  75.62\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.83      0.78      0.80       665\n","           1       0.62      0.65      0.63       664\n","           2       0.66      0.66      0.66       988\n","           3       0.62      0.68      0.65       971\n","           4       0.92      0.87      0.89      1881\n","\n","   micro avg       0.76      0.76      0.76      5169\n","   macro avg       0.73      0.73      0.73      5169\n","weighted avg       0.76      0.76      0.76      5169\n"," samples avg       0.76      0.76      0.76      5169\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1401.047655582428\n","\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,675\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 23:16, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.075600</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.931600</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.890800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.824700</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.763900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.733900</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.658600</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.590300</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.607200</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.567100</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.439900</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.466000</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.409200</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.324000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.309200</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.310000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  74.35\n","F1 weighted:  74.56\n","f1 macro:  71.29\n","f1 micro:  74.35\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.78      0.75      0.77       665\n","           1       0.59      0.63      0.61       663\n","           2       0.68      0.69      0.69       988\n","           3       0.61      0.63      0.62       972\n","           4       0.90      0.86      0.88      1881\n","\n","   micro avg       0.74      0.74      0.74      5169\n","   macro avg       0.71      0.71      0.71      5169\n","weighted avg       0.75      0.74      0.75      5169\n"," samples avg       0.74      0.74      0.74      5169\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1398.311951160431\n","\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,676\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 23:33, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.078300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.904600</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.880800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.750400</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.738500</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.712900</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.629100</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.556900</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.564700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.509500</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.395100</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.401300</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.387200</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.292900</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.260500</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.240300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  75.19\n","F1 weighted:  75.42\n","f1 macro:  72.07\n","f1 micro:  75.19\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.80      0.72      0.76       666\n","           1       0.58      0.67      0.62       663\n","           2       0.71      0.62      0.66       988\n","           3       0.63      0.72      0.67       971\n","           4       0.91      0.88      0.89      1880\n","\n","   micro avg       0.75      0.75      0.75      5168\n","   macro avg       0.72      0.72      0.72      5168\n","weighted avg       0.76      0.75      0.75      5168\n"," samples avg       0.75      0.75      0.75      5168\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1415.028287410736\n","\n","{0: [0.7455987618494874, 0.7490540046155097], 1: [0.7531437415360804, 0.7542148031989206], 2: [0.7562391178177598, 0.7589214098422161], 3: [0.7434706906558328, 0.7455857862260998], 4: [0.7519349845201239, 0.7542060991855495]}\n"]}],"source":["transformer_train(name_model, labels, X, y_onehot, skf, class_weights)"]},{"cell_type":"markdown","metadata":{"id":"5RbV-HnDMLLT"},"source":["## No token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1Nb8fsIMSqJ"},"outputs":[],"source":["X, y, X_encode, y_onehot, num_class, labels, class_weights = get_train_test(df_star, word = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2142420,"status":"ok","timestamp":1714919082742,"user":{"displayName":"Thư Phạm Anh","userId":"01186629817714377316"},"user_tz":-420},"id":"BafVL6RCKZMY","outputId":"7c2fed73-3416-47b6-8f20-a1c4f80dee43"},"outputs":[{"name":"stdout","output_type":"stream","text":["                    Mean accuracy  F1 Score    Recall  Training time\n","Model Name                                                          \n","MultinomialNB            0.289081  0.223858  0.289081       1.858924\n","LogisticRegression       0.619061  0.608296  0.619061      89.133681\n","LinearSVC                0.616894  0.604787  0.619873    2051.396817\n"]}],"source":["ml_train(X_encode, y, skf)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208454,"status":"ok","timestamp":1714919377530,"user":{"displayName":"Thư Phạm Anh","userId":"01186629817714377316"},"user_tz":-420},"id":"ZxwIZhlfKdZs","outputId":"845412e5-bba4-4b9e-8f89-f536f5728691"},"outputs":[{"name":"stdout","output_type":"stream","text":["162/162 [==============================] - 1s 4ms/step\n","162/162 [==============================] - 1s 3ms/step\n","162/162 [==============================] - 1s 3ms/step\n","162/162 [==============================] - 1s 3ms/step\n","162/162 [==============================] - 1s 4ms/step\n","Average Accuracy:  0.6164680247270733\n","Average F1 Score:  0.6068777806380009\n","Average Recall:  0.6164680247270733\n","Training time:  207.25830078125\n"]}],"source":["dl_train(X_encode, y, skf, num_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5PA8X7yPdgi"},"outputs":[],"source":["labels = [str(i) for i in labels]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6716591,"status":"ok","timestamp":1714963651729,"user":{"displayName":"Thuy Bùi Thị Anh","userId":"10901825718356120634"},"user_tz":-420},"id":"OpR8y7EOynXV","outputId":"f4eb2a1a-a221-4aed-d9ee-afe2501c3b93"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,675\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='388' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 388/1620 05:20 < 17:04, 1.20 it/s, Epoch 1.19/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.163300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.959900</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.893100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 23:01, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.163300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.959900</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.893100</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.802500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.802100</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.749700</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.688300</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.613600</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.624300</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.558500</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.459800</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.465600</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.451700</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.339800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.332700</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.314900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  75.6\n","F1 weighted:  75.82\n","f1 macro:  72.77\n","f1 micro:  75.6\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.81      0.78      0.80       665\n","           1       0.60      0.66      0.63       664\n","           2       0.68      0.63      0.66       988\n","           3       0.63      0.70      0.66       971\n","           4       0.91      0.88      0.89      1881\n","\n","   micro avg       0.76      0.76      0.76      5169\n","   macro avg       0.73      0.73      0.73      5169\n","weighted avg       0.76      0.76      0.76      5169\n"," samples avg       0.76      0.76      0.76      5169\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1383.6201651096344\n","\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,675\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 22:50, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.100000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.921200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.891200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.816500</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.787600</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.754600</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.689500</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.652500</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.628200</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.569200</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.476400</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.472200</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.445200</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.355800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.331900</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.347700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  74.81\n","F1 weighted:  75.02\n","f1 macro:  71.35\n","f1 micro:  74.81\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.77      0.77      0.77       665\n","           1       0.56      0.60      0.58       664\n","           2       0.69      0.62      0.65       988\n","           3       0.62      0.71      0.67       971\n","           4       0.92      0.88      0.90      1881\n","\n","   micro avg       0.75      0.75      0.75      5169\n","   macro avg       0.71      0.72      0.71      5169\n","weighted avg       0.75      0.75      0.75      5169\n"," samples avg       0.75      0.75      0.75      5169\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1372.0061905384064\n","\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,675\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 23:02, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.103800</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.919200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.872700</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.795200</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.765600</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.731900</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.668100</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.571400</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.593600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.558400</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.421900</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.429600</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.425900</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.321100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.283200</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.289700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  75.08\n","F1 weighted:  75.37\n","f1 macro:  72.24\n","f1 micro:  75.08\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.80      0.80      0.80       665\n","           1       0.61      0.62      0.62       664\n","           2       0.65      0.66      0.66       988\n","           3       0.62      0.68      0.65       971\n","           4       0.92      0.86      0.89      1881\n","\n","   micro avg       0.75      0.75      0.75      5169\n","   macro avg       0.72      0.72      0.72      5169\n","weighted avg       0.76      0.75      0.75      5169\n"," samples avg       0.75      0.75      0.75      5169\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1385.036476135254\n","\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,675\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 23:09, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.147100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.923000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.899900</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.839700</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.782800</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.745000</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.697300</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.648000</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.676100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.615700</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.509200</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.540800</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.488100</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.400600</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.386300</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.392100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  73.3\n","F1 weighted:  73.6\n","f1 macro:  70.33\n","f1 micro:  73.3\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.77      0.77      0.77       665\n","           1       0.58      0.62      0.60       663\n","           2       0.67      0.63      0.65       988\n","           3       0.59      0.66      0.62       972\n","           4       0.91      0.85      0.88      1881\n","\n","   micro avg       0.73      0.73      0.73      5169\n","   macro avg       0.70      0.71      0.70      5169\n","weighted avg       0.74      0.73      0.74      5169\n"," samples avg       0.73      0.73      0.73      5169\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1391.1552665233612\n","\n"]},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/vocab.txt\n","loading file bpe.codes from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/bpe.codes\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/tokenizer.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"vinai/phobert-base-v2\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"5\",\n","    \"1\": \"1\",\n","    \"2\": \"2\",\n","    \"3\": \"3\",\n","    \"4\": \"4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"1\": 1,\n","    \"2\": 2,\n","    \"3\": 3,\n","    \"4\": 4,\n","    \"5\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.40.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--vinai--phobert-base-v2/snapshots/2b51e367d92093c9688112098510e6a58bab67cd/pytorch_model.bin\n","Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using auto half precision backend\n","***** Running training *****\n","  Num examples = 20,676\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1,620\n","  Number of trainable parameters = 135,002,117\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1620/1620 22:47, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.134400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.952300</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.921400</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.800800</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.786100</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.769400</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.684900</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.618700</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.603600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.583200</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.461500</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.454000</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.445200</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.337200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.315500</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.295700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/checkpoint-500\n","Configuration saved in /content/checkpoint-500/config.json\n","Model weights saved in /content/checkpoint-500/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1000\n","Configuration saved in /content/checkpoint-1000/config.json\n","Model weights saved in /content/checkpoint-1000/model.safetensors\n","Saving model checkpoint to /content/checkpoint-1500\n","Configuration saved in /content/checkpoint-1500/config.json\n","Model weights saved in /content/checkpoint-1500/model.safetensors\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy:  75.19\n","F1 weighted:  75.45\n","f1 macro:  72.02\n","f1 micro:  75.19\n","----------------------------------------------------------------------------------------------------\n","              precision    recall  f1-score   support\n","\n","           5       0.78      0.74      0.76       666\n","           1       0.58      0.65      0.61       663\n","           2       0.71      0.62      0.66       988\n","           3       0.62      0.74      0.68       971\n","           4       0.92      0.87      0.89      1880\n","\n","   micro avg       0.75      0.75      0.75      5168\n","   macro avg       0.72      0.72      0.72      5168\n","weighted avg       0.76      0.75      0.75      5168\n"," samples avg       0.75      0.75      0.75      5168\n","\n","\n","\n","\n","====================================================================================================\n","\n","Training time:  1369.8560526371002\n","\n","{0: [0.7560456568001548, 0.7582189943761163], 1: [0.7481137550783518, 0.7501821572995866], 2: [0.750822209324821, 0.7537166512845969], 3: [0.7330237957051654, 0.7359999555457174], 4: [0.7519349845201239, 0.7545384835742145]}\n"]}],"source":["transformer_train(name_model, labels, X, y_onehot, skf, class_weights)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["0Jp02BmqRg3j","F5La1eLNRl7e","k_63NDZJSwoF","_DQWQn0ipo57","Y3xC09gWpx3o","LW8y-8Mnq2G_","d0_SAZAWq6yP","7Vhkr7fX9-e9","HsGsUeFILNrR","FesbhUCmLcNy","j9igcA7gLevQ","GNzwNA1TMJRw"],"gpuType":"T4","provenance":[{"file_id":"1XUWapxAoohCUOENQZKgitpz4NBzWr_KH","timestamp":1714901418627}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}